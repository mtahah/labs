# Enhanced Docker Swarm Lab with Learning Insights

## Objectives
By the end of this lab, students will be able to:

- **Understand** the fundamentals of Docker Swarm orchestration and how it differs from standalone Docker
- **Initialize** a Docker Swarm cluster using `docker swarm init` with proper networking
- **Add** worker nodes to a Swarm cluster using `docker swarm join` with security tokens
- **Deploy** multi-container applications using Docker Stack with overlay networking
- **Scale** services within a Swarm cluster for high availability and load distribution
- **Monitor** and inspect Swarm services and nodes for troubleshooting and optimization
- **Understand** the benefits of container orchestration for production environments

## Prerequisites
Before starting this lab, students should have:

- Basic understanding of Docker containers and images
- Familiarity with Docker CLI commands
- Knowledge of YAML file structure
- Understanding of basic networking concepts
- Experience with Linux command line operations

## Lab Environment Setup
**Ready-to-Use Cloud Machines**: Al Nafi provides pre-configured Linux-based cloud machines for this lab. Simply click Start Lab to access your environment - no need to build your own virtual machines or install Docker manually.

Your lab environment includes:
- **3 Ubuntu Linux machines** with Docker pre-installed
  - **Machine 1**: `manager` (will serve as Swarm manager)
  - **Machine 2**: `worker1` (will serve as Swarm worker)
  - **Machine 3**: `worker2` (will serve as Swarm worker)
- All necessary networking configured between machines

---

## Task 1: Initialize a Docker Swarm Cluster

### üéØ Learning Focus: Understanding Docker Swarm Architecture

**Key Concept**: Docker Swarm transforms individual Docker hosts into a cluster that can be managed as a single virtual system. The Swarm consists of:
- **Manager nodes**: Control the cluster, maintain cluster state, and schedule services
- **Worker nodes**: Execute the containers/tasks assigned by managers

### Subtask 1.1: Connect to the Manager Node

Access your manager machine through the provided terminal.

**üí° Learning Tip**: Always verify your environment before starting. This helps identify potential issues early.

```bash
# Verify Docker installation and version
docker --version
```

**Expected Output**:
```
Docker version 24.0.7, build afdd53b
```

```bash
# Check Docker system information and current state
docker info
```

**Key Output Sections to Notice**:
```
Server:
 Context:    default
 Debug Mode: false
 ...
 Swarm: inactive    # ‚Üê This shows Swarm is not yet enabled
 ...
```

**üéØ Learning Check**: Let's specifically check Swarm status:
```bash
docker info | grep Swarm
```

**Expected Output**:
```
Swarm: inactive
```

**üìö Why This Matters**: The `inactive` status confirms we're starting with a standalone Docker installation. Once we initialize Swarm, this will change to `active`.

### Subtask 1.2: Initialize the Swarm Cluster

**Key Concept**: The `docker swarm init` command transforms your Docker host into a Swarm manager node and creates a new cluster.

**üí° Learning Tip**: The `--advertise-addr` flag is crucial - it tells other nodes which IP address to use when connecting to this manager.

```bash
# Initialize Docker Swarm with automatic IP detection
echo "üöÄ Initializing Docker Swarm cluster..."
docker swarm init --advertise-addr $(hostname -I | awk '{print $1}')
```

**Expected Output**:
```
Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex-dq3n5j4lg0o8b5538oigjl4jd 192.168.1.100:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

**üîë Important**: Save this join command! You'll need it in the next task.

**üìö Understanding the Output**:
- **Node ID**: `dxn1zf6l61qsb1josjja83ngz` - Unique identifier for this manager node
- **Join Token**: `SWMTKN-1-49nj1cmql0jkz5s954yi3oex-dq3n5j4lg0o8b5538oigjl4jd` - Security token for workers
- **Manager Address**: `192.168.1.100:2377` - Where workers connect (port 2377 is Swarm's management port)

### Subtask 1.3: Verify Swarm Initialization

**Learning Focus**: Verification is a critical DevOps practice. Always confirm your changes took effect.

```bash
# Check if Swarm mode is now active
echo "üîç Checking Swarm status..."
docker info | grep Swarm
```

**Expected Output**:
```
Swarm: active
```

**üí° Success Indicator**: The status changed from `inactive` to `active`!

```bash
# List all nodes in the Swarm cluster
echo "üìã Listing cluster nodes..."
docker node ls
```

**Expected Output**:
```
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dxn1zf6l61qsb1josjja83ngz *   manager    Ready     Active         Leader           24.0.7
```

**üìö Understanding Node Status**:
- **ID**: Unique node identifier
- **HOSTNAME**: Node's hostname (manager)
- **STATUS**: `Ready` means the node is operational
- **AVAILABILITY**: `Active` means it can receive tasks
- **MANAGER STATUS**: `Leader` indicates this is the primary manager
- **ENGINE VERSION**: Docker version on this node
- **Asterisk (*)**: Indicates the current node you're connected to

---

## Task 2: Add Additional Nodes to the Cluster

### üéØ Learning Focus: Understanding Swarm Security and Node Roles

**Key Concept**: Swarm uses cryptographic tokens for security. Worker and manager tokens are different to maintain the principle of least privilege.

### Subtask 2.1: Retrieve Join Tokens

**üí° Learning Tip**: Tokens can be regenerated if compromised using `docker swarm join-token --rotate worker`.

```bash
# Get the worker join token (run on manager node)
echo "üîë Retrieving worker join token..."
docker swarm join-token worker
```

**Expected Output**:
```
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex-dq3n5j4lg0o8b5538oigjl4jd 192.168.1.100:2377
```

**üîí Security Note**: This token grants worker-level access only. For manager tokens, use `docker swarm join-token manager`.

### Subtask 2.2: Join Worker Node 1

Connect to your **worker1** machine.

```bash
# Verify Docker is running on worker1
echo "‚úÖ Verifying Docker installation on worker1..."
docker --version
```

**Expected Output**:
```
Docker version 24.0.7, build afdd53b
```

```bash
# Join the Swarm cluster as a worker
echo "ü§ù Joining Swarm cluster as worker1..."
docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex-dq3n5j4lg0o8b5538oigjl4jd 192.168.1.100:2377
```
*Replace the token and IP with your actual values from the manager output*

**Expected Output**:
```
This node joined a swarm as a worker.
```

**üìö What Just Happened**: 
- Worker1 authenticated using the security token
- It registered itself with the manager node
- The manager added it to the cluster database
- Worker1 is now ready to receive and execute tasks

### Subtask 2.3: Join Worker Node 2

Connect to your **worker2** machine.

```bash
# Join worker2 to the Swarm
echo "ü§ù Joining Swarm cluster as worker2..."
docker swarm join --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex-dq3n5j4lg0o8b5538oigjl4jd 192.168.1.100:2377
```

**Expected Output**:
```
This node joined a swarm as a worker.
```

### Subtask 2.4: Verify All Nodes Joined

Return to the **manager** node.

```bash
# List all nodes in the cluster
echo "üèóÔ∏è Verifying complete cluster topology..."
docker node ls
```

**Expected Output**:
```
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dxn1zf6l61qsb1josjja83ngz *   manager    Ready     Active         Leader           24.0.7
x1zf6l61qsb1josjja83ngza     worker1    Ready     Active                          24.0.7
y2zf6l61qsb1josjja83ngzb     worker2    Ready     Active                          24.0.7
```

**üéâ Success Indicators**:
- **3 nodes total**: 1 manager + 2 workers
- **All nodes "Ready"**: Can accept and execute tasks
- **All nodes "Active"**: Available for scheduling
- **Manager shows "Leader"**: Primary decision-maker for the cluster

---

## Task 3: Deploy a Multi-Container Service Using Docker Stack

### üéØ Learning Focus: Understanding Service Orchestration and Overlay Networks

**Key Concepts**:
- **Docker Stack**: Manages multi-service applications as a single unit
- **Overlay Networks**: Allow containers across different hosts to communicate securely
- **Service Placement**: Control where containers run using constraints

### Subtask 3.1: Create a Docker Compose File

**üí° Learning Tip**: Stacks use the same Compose file format but add orchestration features like replicas, placement constraints, and restart policies.

```bash
# Create project directory
echo "üìÅ Setting up project structure..."
mkdir ~/swarm-lab
cd ~/swarm-lab
echo "Current directory: $(pwd)"
```

**Expected Output**:
```
Current directory: /home/ubuntu/swarm-lab
```

```bash
# Create comprehensive Docker Compose file for the stack
echo "üìù Creating Docker Compose file with orchestration features..."
cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  web:
    image: nginx:alpine
    ports:
      - "80:80"
    deploy:
      replicas: 3                    # Run 3 instances for load balancing
      restart_policy:
        condition: on-failure        # Restart if container fails
      placement:
        constraints:
          - node.role == worker      # Only run on worker nodes
    volumes:
      - web-content:/usr/share/nginx/html
    networks:
      - webnet

  redis:
    image: redis:alpine
    deploy:
      replicas: 1                    # Single instance (stateful service)
      restart_policy:
        condition: on-failure
    networks:
      - webnet                       # Same network for service communication

  visualizer:
    image: dockersamples/visualizer:stable
    ports:
      - "8080:8080"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"  # Access Docker API
    deploy:
      placement:
        constraints:
          - node.role == manager     # Run on manager for Docker socket access
    networks:
      - webnet

volumes:
  web-content:                       # Shared storage across nodes

networks:
  webnet:
    driver: overlay                  # Multi-host networking
EOF

echo "‚úÖ Compose file created successfully!"
```

**üìö Configuration Deep Dive**:

**Web Service**:
- **3 replicas**: Provides load balancing and high availability
- **Worker constraint**: Separates application workload from management
- **Nginx**: Lightweight web server perfect for demonstrations

**Redis Service**:
- **1 replica**: Stateful services typically run single instances
- **No placement constraint**: Can run on any available node

**Visualizer Service**:
- **Manager constraint**: Needs Docker socket access for cluster visualization
- **Port 8080**: Web interface to see real-time cluster state

**Overlay Network**:
- **Cross-host communication**: Containers can talk across different machines
- **Encrypted by default**: Secure communication between services

### Subtask 3.2: Deploy the Stack

**üí° Learning Tip**: Stack deployment is atomic - either all services deploy successfully, or none do.

```bash
# Deploy the complete application stack
echo "üöÄ Deploying multi-service application stack..."
docker stack deploy -c docker-compose.yml webapp
```

**Expected Output**:
```
Creating network webapp_webnet
Creating service webapp_web
Creating service webapp_redis
Creating service webapp_visualizer
```

**üìö What's Happening**:
1. **Network Creation**: Overlay network `webapp_webnet` spans all nodes
2. **Service Creation**: Each service gets registered in the cluster
3. **Task Scheduling**: Manager decides which nodes run which containers
4. **Container Starting**: Docker pulls images and starts containers on assigned nodes

```bash
# Verify stack deployment
echo "üìä Checking deployed stacks..."
docker stack ls
```

**Expected Output**:
```
NAME      SERVICES   ORCHESTRATOR
webapp    3          Swarm
```

```bash
# List services within the stack
echo "üîç Examining stack services..."
docker stack services webapp
```

**Expected Output**:
```
ID             NAME                MODE         REPLICAS   IMAGE                             PORTS
abc123def456   webapp_redis        replicated   1/1        redis:alpine                      
def456ghi789   webapp_visualizer   replicated   1/1        dockersamples/visualizer:stable   *:8080->8080/tcp
ghi789jkl012   webapp_web          replicated   3/3        nginx:alpine                      *:80->80/tcp
```

**üìä Understanding Service Status**:
- **REPLICAS**: Shows desired vs actual running containers (3/3 means all 3 are running)
- **MODE**: `replicated` means multiple instances, `global` would mean one per node
- **PORTS**: `*:80->80/tcp` means accessible on port 80 from any node

### Subtask 3.3: Verify Service Deployment

```bash
# Get detailed status of all services
echo "üîç Detailed service inspection..."
docker service ls
```

**Expected Output**:
```
ID             NAME                MODE         REPLICAS   IMAGE                             PORTS
abc123def456   webapp_redis        replicated   1/1        redis:alpine                      
def456ghi789   webapp_visualizer   replicated   1/1        dockersamples/visualizer:stable   *:8080->8080/tcp
ghi789jkl012   webapp_web          replicated   3/3        nginx:alpine                      *:80->80/tcp
```

```bash
# See which nodes are running web service containers
echo "üó∫Ô∏è  Web service distribution across nodes..."
docker service ps webapp_web
```

**Expected Output**:
```
ID             NAME           IMAGE          NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
mno345pqr678   webapp_web.1   nginx:alpine   worker1   Running         Running 2 minutes ago             
pqr678stu901   webapp_web.2   nginx:alpine   worker2   Running         Running 2 minutes ago             
stu901vwx234   webapp_web.3   nginx:alpine   worker1   Running         Running 2 minutes ago             
```

**üìä Distribution Analysis**:
- Notice how containers are distributed across worker nodes
- Swarm's scheduler automatically balances the load
- If a node fails, containers will be rescheduled to healthy nodes

```bash
# Check all service distributions
echo "üåê Complete service topology..."
docker service ps webapp_web webapp_redis webapp_visualizer
```

---

## Task 4: Scale Services Within the Swarm

### üéØ Learning Focus: Dynamic Scaling and Load Distribution

**Key Concept**: Scaling is one of orchestration's biggest advantages. You can adjust capacity without downtime or manual container management.

### Subtask 4.1: Scale the Web Service

```bash
# Check current service scale
echo "üìä Current service scaling status..."
docker service ls
```

**üí° Observation**: Note the REPLICAS column showing current scale.

```bash
# Scale web service to handle more traffic
echo "üìà Scaling web service from 3 to 5 replicas..."
docker service scale webapp_web=5
```

**Expected Output**:
```
webapp_web scaled to 5
overall progress: 5 out of 5 tasks 
1/5: running   [==================================================>] 
2/5: running   [==================================================>] 
3/5: running   [==================================================>] 
4/5: running   [==================================================>] 
5/5: running   [==================================================>] 
verify: Service converged
```

**üìö Scaling Process**:
1. **Command Received**: Manager processes the scale request
2. **Task Creation**: 2 new tasks created (5 desired - 3 current = 2 new)
3. **Scheduling**: Manager assigns new tasks to available nodes
4. **Container Creation**: New containers start on assigned nodes
5. **Load Balancer Update**: Traffic automatically distributes to all 5 instances

```bash
# Monitor the scaling process in real-time
echo "‚è±Ô∏è  Monitoring scaling progress..."
docker service ps webapp_web
```

**Expected Output**:
```
ID             NAME           IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
mno345pqr678   webapp_web.1   nginx:alpine   worker1   Running         Running 5 minutes ago              
pqr678stu901   webapp_web.2   nginx:alpine   worker2   Running         Running 5 minutes ago              
stu901vwx234   webapp_web.3   nginx:alpine   worker1   Running         Running 5 minutes ago              
vwx234yza567   webapp_web.4   nginx:alpine   worker2   Running         Running 30 seconds ago             
yza567bcd890   webapp_web.5   nginx:alpine   worker1   Running         Running 30 seconds ago             
```

```bash
# Verify new replica count
echo "‚úÖ Verifying scale operation success..."
docker service ls
```

**Expected Output**:
```
ID             NAME                MODE         REPLICAS   IMAGE                             PORTS
abc123def456   webapp_redis        replicated   1/1        redis:alpine                      
def456ghi789   webapp_visualizer   replicated   1/1        dockersamples/visualizer:stable   *:8080->8080/tcp
ghi789jkl012   webapp_web          replicated   5/5        nginx:alpine                      *:80->80/tcp
```

### Subtask 4.2: Scale Multiple Services

**üí° Learning Tip**: You can scale multiple services simultaneously, which is useful during traffic spikes or maintenance windows.

```bash
# Scale multiple services in one command
echo "üîÑ Scaling multiple services simultaneously..."
docker service scale webapp_web=2 webapp_redis=2
```

**Expected Output**:
```
webapp_web scaled to 2
webapp_redis scaled to 2
overall progress: 2 out of 2 tasks 
1/2: running   [==================================================>] 
2/2: running   [==================================================>] 
verify: Service converged 

overall progress: 2 out of 2 tasks 
1/2: running   [==================================================>] 
2/2: running   [==================================================>] 
verify: Service converged
```

```bash
# Check updated service status
echo "üìä Updated service scaling status..."
docker service ls
```

**Expected Output**:
```
ID             NAME                MODE         REPLICAS   IMAGE                             PORTS
abc123def456   webapp_redis        replicated   2/2        redis:alpine                      
def456ghi789   webapp_visualizer   replicated   1/1        dockersamples/visualizer:stable   *:8080->8080/tcp
ghi789jkl012   webapp_web          replicated   2/2        nginx:alpine                      *:80->80/tcp
```

```bash
# See updated distribution
echo "üó∫Ô∏è  Updated service distribution..."
docker service ps webapp_web webapp_redis
```

### Subtask 4.3: Test Service High Availability

**üéØ Learning Focus**: Understanding Swarm's Self-Healing Capabilities

**Key Concept**: Swarm continuously monitors container health and automatically reschedules failed containers to maintain desired state.

```bash
# First, identify which containers are running where
echo "üîç Current service distribution before failure simulation..."
docker service ps webapp_web
```

**üìã Note**: Record which worker node has running containers.

```bash
# Check current node status
echo "üè• Cluster health status..."
docker node ls
```

Now, connect to **worker1** or **worker2** (whichever has running containers):

```bash
# Simulate node failure by stopping Docker service
echo "‚ö†Ô∏è  Simulating node failure by stopping Docker..."
sudo systemctl stop docker
echo "Docker service stopped on this node"
```

Return to the **manager** node:

```bash
# Observe Swarm's automatic recovery response
echo "üö® Monitoring Swarm's response to node failure..."
docker service ps webapp_web
```

**Expected Output** (after a few moments):
```
ID             NAME           IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
abc123def456   webapp_web.1   nginx:alpine   worker2   Running         Running 1 minute ago               
def456ghi789   webapp_web.2   nginx:alpine   worker2   Running         Running 30 seconds ago             
xyz789old111   webapp_web.1   nginx:alpine   worker1   Shutdown        Failed 45 seconds ago    "task: non-zero exit (125)"
```

**üìö What You're Seeing**:
- **New containers**: Started on healthy nodes to maintain desired state
- **Failed containers**: Marked as "Shutdown" with failure reason
- **Automatic rescheduling**: No manual intervention required

```bash
# Check node status during failure
echo "üè• Node status during failure..."
docker node ls
```

**Expected Output**:
```
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dxn1zf6l61qsb1josjja83ngz *   manager    Ready     Active         Leader           24.0.7
x1zf6l61qsb1josjja83ngza     worker1    Down      Active                          24.0.7
y2zf6l61qsb1josjja83ngzb     worker2    Ready     Active                          24.0.7
```

**üîç Notice**: worker1 shows "Down" status but remains "Active" (will rejoin when healthy).

Now, return to the affected worker node:

```bash
# Restore the "failed" node
echo "üîß Restoring failed node..."
sudo systemctl start docker
echo "Docker service restarted"
```

Return to the **manager** node:

```bash
# Verify node recovery
echo "üíö Verifying node recovery..."
docker node ls
```

**Expected Output**:
```
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dxn1zf6l61qsb1josjja83ngz *   manager    Ready     Active         Leader           24.0.7
x1zf6l61qsb1josjja83ngza     worker1    Ready     Active                          24.0.7
y2zf6l61qsb1josjja83ngzb     worker2    Ready     Active                          24.0.7
```

**üéâ Recovery Complete**: All nodes show "Ready" status again.

---

## Task 5: Inspect the State of Services and Nodes

### üéØ Learning Focus: Monitoring and Troubleshooting Production Clusters

**Key Concept**: Effective operations require comprehensive visibility into cluster state, service health, and resource utilization.

### Subtask 5.1: Comprehensive Service Inspection

```bash
# Get overview of all services across all stacks
echo "üìä Complete service inventory..."
docker service ls
```

```bash
# Deep dive into web service configuration
echo "üîç Detailed web service inspection..."
docker service inspect webapp_web
```

**üí° Learning Tip**: This JSON output contains everything about the service - networking, placement, resources, update config, etc.

```bash
# Get a human-readable format
echo "üìñ Human-readable service configuration..."
docker service inspect --pretty webapp_web
```

**Expected Output** (excerpt):
```
ID:             ghi789jkl012mno3
Name:           webapp_web
Service Mode:   Replicated
 Replicas:      2
Placement:
 Constraints:   [node.role == worker]
UpdateConfig:
 Parallelism:   1
 On failure:    pause
Platform:       linux
ForceUpdate:    0
ContainerSpec:
 Image:         nginx:alpine
 Init:          false
Networks:       webapp_webnet
Mounts:
  Target:       /usr/share/nginx/html
  Source:       webapp_web-content
  Type:         volume
Resources:
Ports:
 PublishedPort = 80
  Protocol = tcp
  TargetPort = 80
  PublishMode = ingress
```

```bash
# View service logs for troubleshooting
echo "üìú Recent service logs..."
docker service logs --tail 10 webapp_web
```

**Expected Output**:
```
webapp_web.1.abc123@worker2    | 10.0.0.3 - - [17/Aug/2025:10:30:45 +0000] "GET / HTTP/1.1" 200 615
webapp_web.2.def456@worker1    | 10.0.0.4 - - [17/Aug/2025:10:30:46 +0000] "GET / HTTP/1.1" 200 615
```

### Subtask 5.2: Node Management and Inspection

```bash
# Comprehensive node listing with all details
echo "üèóÔ∏è  Complete cluster node inventory..."
docker node ls
```

```bash
# Deep dive into worker1 configuration
echo "üîç Detailed node inspection - worker1..."
docker node inspect worker1
```

```bash
# Human-readable node information
echo "üìñ Worker1 readable configuration..."
docker node inspect --pretty worker1
```

**Expected Output** (excerpt):
```
ID:                     x1zf6l61qsb1josjja83ngza
Hostname:               worker1
Joined at:              2025-08-17 10:15:30.123456789 +0000 utc
Status:
 State:                 Ready
 Availability:          Active
 Address:               192.168.1.101
Platform:
 Architecture:          x86_64
 OS:                    linux
Resources:
 CPUs:                  2
 Memory:                2.0GiB
Plugins:
 Log:                   awslogs, fluentd, gcplogs, gelf, journald, json-file, local, logentries, splunk, syslog
 Network:               bridge, host, ipvlan, macvlan, null, overlay
 Volume:                local
Engine Version:         24.0.7
TLS Info:
 TrustRoot:
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
```

```bash
# Check resource usage across the cluster
echo "üíæ Cluster resource utilization..."
docker system df
```

**Expected Output**:
```
TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              3                   3                   45.2MB              0B (0%)
Containers          4                   4                   1.23kB              0B (0%)
Local Volumes       1                   1                   0B                  0B
Build Cache         0                   0                   0B                  0B
```

```bash
# Monitor real-time cluster events
echo "üîÑ Recent cluster events (last 10 minutes)..."
docker system events --since 10m --until 1s
```

### Subtask 5.3: Network and Volume Inspection

**üí° Learning Tip**: Understanding Swarm networking is crucial for troubleshooting connectivity issues.

```bash
# List all networks (including overlay networks)
echo "üåê Complete network inventory..."
docker network ls
```

**Expected Output**:
```
NETWORK ID     NAME              DRIVER    SCOPE
abc123def456   bridge            bridge    local
def456ghi789   docker_gwbridge   bridge    local
ghi789jkl012   host              host      local
jkl012mno345   none              null      local
mno345pqr678   webapp_webnet     overlay   swarm
```

**üìö Network Types**:
- **bridge**: Default single-host networking
- **overlay**: Multi-host Swarm networking (notice `swarm` scope)
- **host**: Direct host networking
- **docker_gwbridge**: Gateway between overlay and host networks

```bash
# Inspect the overlay network
echo "üîç Overlay network detailed inspection..."
docker network inspect webapp_webnet
```

**Key Information to Notice**:
- **Scope**: `swarm` - spans multiple nodes
- **Driver**: `overlay` - multi-host networking
- **Subnet**: Automatically assigned IP range
- **Containers**: Shows which containers are connected

```bash
# List volumes created by the stack
echo "üíæ Volume inventory..."
docker volume ls
```

**Expected Output**:
```
DRIVER    VOLUME NAME
local     webapp_web-content
```

```bash
# Inspect volume details
echo "üîç Volume detailed inspection..."
docker volume inspect webapp_web-content
```

### Subtask 5.4: Access the Deployed Application

**üéØ Learning Focus**: Testing Load Balancing and Service Discovery

```bash
# Get manager node IP for access
echo "üåê Finding cluster access points..."
MANAGER_IP=$(hostname -I | awk '{print $1}')
echo "Manager IP: $MANAGER_IP"
echo "Main application URL: http://$MANAGER_IP"
echo "Visualizer URL: http://$MANAGER_IP:8080"
```

```bash
# Test load balancing with multiple requests
echo "üîÑ Testing load balancing..."
for i in {1..5}; do
  echo "Request $i:"
  curl -s http://$MANAGER_IP | grep -o '<title>.*</title>' || echo "Response received"
  sleep 1
done
```

**üí° Load Balancing Test**: Each request might hit a different container. Swarm's built-in load balancer (routing mesh) distributes traffic across all replicas.

```bash
# Check which containers are actually serving requests
echo "üìä Current service distribution handling requests..."
docker service ps webapp_web
```

---

## Advanced Operations

### Rolling Updates

**üéØ Learning Focus**: Zero-Downtime Deployments

**Key Concept**: Rolling updates replace containers one at a time, ensuring service availability during updates.

```bash
# Perform a rolling update to latest nginx image
echo "üîÑ Initiating rolling update..."
docker service update --image nginx:latest webapp_web
```

**Expected Output**:
```
webapp_web
overall progress: 2 out of 2 tasks 
1/2: running   [==================================================>] 
2/2: running   [==================================================>] 
verify: Service converged
```

```bash
# Monitor rolling update progress
echo "üìä Rolling update progress..."
docker service ps webapp_web
```

**Expected Output**:
```
ID             NAME               IMAGE          NODE      DESIRED STATE   CURRENT STATE                ERROR     PORTS
new123abc456   webapp_web.1       nginx:latest   worker1   Running         Running 30 seconds ago               
new456def789   webapp_web.2       nginx:latest   worker2   Running         Running 45 seconds ago               
old789ghi012   webapp_web.1       nginx:alpine   worker1   Shutdown        Shutdown 1 minute ago               
old012jkl345   webapp_web.2       nginx:alpine   worker2   Shutdown        Shutdown 1 minute ago               
```

**üìö Rolling Update Process**:
1. **One-by-one replacement**: Old containers shut down as new ones start
2. **Health checks**: New containers must be healthy before old ones stop
3. **Zero downtime**: Service remains available throughout the process
4. **Rollback capability**: Previous version info retained for potential rollback

### Node Maintenance - Draining Nodes

**üéØ Learning Focus**: Planned Maintenance Without Service Interruption

**Key Concept**: Draining removes all running tasks from a node gracefully, allowing for maintenance while keeping services available.

```bash
# Drain worker1 for maintenance
echo "üîß Draining worker1 for maintenance..."
docker node update --availability drain worker1
```

**Expected Output**:
```
worker1
```

```bash
# Observe how services redistribute
echo "üîÑ Monitoring service redistribution during drain..."
docker service ps webapp_web
```

**Expected Output**:
```
ID             NAME               IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
redistributed1 webapp_web.1       nginx:latest   worker2   Running         Running 30 seconds ago             
redistributed2 webapp_web.2       nginx:latest   worker2   Running         Running 45 seconds ago             
drained001     webapp_web.1       nginx:latest   worker1   Shutdown        Shutdown 1 minute ago              
drained002     webapp_web.2       nginx:latest   worker1   Shutdown        Shutdown 1 minute ago              
```

```bash
# Check node status during drain
echo "üè• Node status during maintenance..."
docker node ls
```

**Expected Output**:
```
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dxn1zf6l61qsb1josjja83ngz *   manager    Ready     Active         Leader           24.0.7
x1zf6l61qsb1josjja83ngza     worker1    Ready     Drain                           24.0.7
y2zf6l61qsb1josjja83ngzb     worker2    Ready     Active                          24.0.7
```

**üìö Drain Status Explanation**:
- **STATUS**: `Ready` - Node is healthy but not accepting new tasks
- **AVAILABILITY**: `Drain` - All existing tasks moved elsewhere
- **No service interruption**: Applications continue running on other nodes

```bash
# Return node to active status after maintenance
echo "‚úÖ Returning worker1 to active status..."
docker node update --availability active worker1
```

**Expected Output**:
```
worker1
```

```bash
# Verify node is back in service
echo "üîÑ Verifying node restoration..."
docker node ls
```

**Expected Output**:
```
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dxn1zf6l61qsb1josjja83ngz *   manager    Ready     Active         Leader           24.0.7
x1zf6l61qsb1josjja83ngza     worker1    Ready     Active                          24.0.7
y2zf6l61qsb1josjja83ngzb     worker2    Ready     Active                          24.0.7
```

**üí° Note**: Existing tasks won't automatically redistribute back to worker1 unless you scale or restart services.

---

## Cleanup and Best Practices

### Remove the Stack

**üéØ Learning Focus**: Proper Resource Management

```bash
# Remove the deployed stack
echo "üßπ Removing application stack..."
docker stack rm webapp
```

**Expected Output**:
```
Removing service webapp_redis
Removing service webapp_visualizer
Removing service webapp_web
Removing network webapp_webnet
```

**üìö Cleanup Process**:
1. **Services stopped**: All containers gracefully shut down
2. **Networks removed**: Overlay network deleted
3. **Volumes preserved**: Data volumes remain for potential reuse (manual cleanup needed)

```bash
# Verify complete stack removal
echo "‚úÖ Verifying stack removal..."
docker stack ls
```

**Expected Output**:
```
NAME      SERVICES   ORCHESTRATOR
```

```bash
# Confirm no services remain
echo "‚úÖ Confirming service cleanup..."
docker service ls
```

**Expected Output**:
```
ID        NAME      MODE      REPLICAS   IMAGE     PORTS
```

```bash
# Check for remaining volumes (cleanup if needed)
echo "üîç Checking for remaining volumes..."
docker volume ls
```

```bash
# Remove leftover volumes if desired
echo "üóëÔ∏è  Cleaning up volumes..."
docker volume prune -f
```

### Leave the Swarm (Optional)

**‚ö†Ô∏è Warning**: Only perform this if you want to completely dismantle the cluster.

```bash
# On worker nodes, leave the Swarm
echo "üëã Worker nodes leaving Swarm..."
```

On **worker1** and **worker2**:
```bash
docker swarm leave
```

**Expected Output**:
```
Node left the swarm.
```

Back on the **manager** node:
```bash
# Remove disconnected worker nodes
echo "üóëÔ∏è  Removing disconnected nodes..."
docker node rm worker1 worker2
```

```bash
# Manager leaves Swarm mode (destroys cluster)
echo "üèÅ Manager leaving Swarm - cluster destruction..."
docker swarm leave --force
```

**Expected Output**:
```
Node left the swarm.
```

---

## Troubleshooting Guide

### Common Issues and Solutions

#### Issue 1: Node Fails to Join Swarm
**Symptoms**:
```
Error response from daemon: rpc error: code = Unavailable desc = connection error
```

**üí° Diagnostic Steps**:
```bash
# Check network connectivity between nodes
ping MANAGER_IP

# Verify required ports are open
nc -zv MANAGER_IP 2377  # Management
nc -zv MANAGER_IP 7946  # Node communication  
nc -zv MANAGER_IP 4789  # Overlay network
```

**üîß Solutions**:
- Ensure firewall allows ports 2377, 7946, and 4789
- Check if manager IP is reachable from worker
- Verify token hasn't expired (regenerate if needed)

#### Issue 2: Services Not Starting
**Symptoms**:
```bash
docker service ls
# Shows 0/3 replicas running
```

**üí° Diagnostic Steps**:
```bash
# Check service logs for errors
docker service logs SERVICE_NAME

# Inspect failed tasks
docker service ps SERVICE_NAME --no-trunc

# Verify image availability
docker service inspect SERVICE_NAME --pretty
```

**üîß Common Solutions**:
- Image pull failures: Check image name/tag
- Resource constraints: Insufficient memory/CPU
- Placement constraints: No nodes match requirements

#### Issue 3: Cannot Access Deployed Application
**Symptoms**: 
- Connection timeouts when accessing application URLs

**üí° Diagnostic Steps**:
```bash
# Verify services are running
docker service ls

# Check port mappings
docker service inspect --pretty webapp_web

# Test from manager node
curl localhost:80
```

**üîß Solutions**:
- Verify port mappings in compose file
- Check if services are running on expected nodes
- Ensure firewall allows external access to published ports

#### Issue 4: Swarm Initialization Fails
**Symptoms**:
```
Error response from daemon: could not choose an IP address to advertise
```

**üí° Solution**:
```bash
# Specify advertise address explicitly
docker swarm init --advertise-addr $(ip route get 1 | awk '{print $7; exit}')
```

### Essential Debugging Commands

```bash
# System-wide Docker information
docker system info

# Real-time events monitoring
docker system events --since 1h

# Node connectivity test
docker node inspect NODE_NAME --format '{{.Status.Addr}}'

# Service task distribution
docker service ps SERVICE_NAME --format 'table {{.Name}}\t{{.Node}}\t{{.CurrentState}}'

# Network connectivity within overlay
docker exec CONTAINER_ID ping SERVICE_NAME

# Resource usage monitoring
docker stats --no-stream
```

---

## Key Learning Outcomes Summary

### üéØ Core Concepts Mastered

**1. Swarm Architecture Understanding**:
- **Manager vs Worker roles**: Decision-making vs task execution
- **High availability**: Multiple managers prevent single points of failure
- **Consensus algorithm**: Raft ensures consistent cluster state

**2. Service Orchestration**:
- **Declarative model**: "I want 3 replicas" vs manual container management
- **Desired state**: Swarm continuously works to maintain your specifications
- **Self-healing**: Automatic replacement of failed containers

**3. Networking Mastery**:
- **Overlay networks**: Secure multi-host container communication
- **Service discovery**: Containers find each other by service name
- **Load balancing**: Automatic traffic distribution via routing mesh

**4. Operational Excellence**:
- **Rolling updates**: Zero-downtime deployments
- **Scaling strategies**: Horizontal scaling for increased capacity
- **Drain procedures**: Planned maintenance without service interruption

### üèÜ Production-Ready Skills Acquired

**DevOps Engineers**:
- ‚úÖ Multi-environment orchestration (dev/staging/prod)
- ‚úÖ Service monitoring and log aggregation
- ‚úÖ Automated deployment pipelines
- ‚úÖ Disaster recovery procedures

**System Administrators**:
- ‚úÖ Cluster health monitoring
- ‚úÖ Resource utilization optimization  
- ‚úÖ Security token management
- ‚úÖ Network troubleshooting

**Developers**:
- ‚úÖ Application decomposition into services
- ‚úÖ Service dependency management
- ‚úÖ Configuration externalization
- ‚úÖ Development-to-production consistency

### üöÄ Real-World Applications

**Production Deployment Scenarios**:
- **E-commerce platforms**: Scale web servers during traffic spikes
- **Microservices architectures**: Manage complex service interactions
- **CI/CD pipelines**: Automated testing and deployment workflows
- **High-availability systems**: Ensure 99.9%+ uptime requirements

**Career Advancement**:
- **Docker Certified Associate (DCA)**: This lab covers core orchestration topics
- **Kubernetes transition**: Swarm concepts apply to Kubernetes
- **Cloud orchestration**: Understanding for ECS, AKS, GKE services
- **Site Reliability Engineering**: Foundation for SRE practices

---

## Next Steps and Further Learning

### üéì Advanced Topics to Explore

**1. Security Hardening**:
- Certificate rotation and management
- Secrets management with Docker Secrets
- Network segmentation strategies
- Role-based access control (RBAC)

**2. Monitoring and Observability**:
- Integration with Prometheus and Grafana
- Centralized logging with ELK stack
- Distributed tracing for microservices
- Application performance monitoring (APM)

**3. Production Patterns**:
- Blue-green deployments
- Canary releases
- Circuit breaker patterns
- Service mesh integration

### üìö Recommended Resources

**Official Documentation**:
- [Docker Swarm Documentation](https://docs.docker.com/engine/swarm/)
- [Docker Compose File Reference](https://docs.docker.com/compose/compose-file/)
- [Docker CLI Command Reference](https://docs.docker.com/engine/reference/commandline/)

**Certification Preparation**:
- Docker Certified Associate (DCA) Study Guide
- Practice labs and mock exams
- Community study groups and forums

**Related Technologies**:
- Kubernetes for advanced orchestration
- HashiCorp Nomad for alternative orchestration
- Service mesh technologies (Istio, Linkerd)

---

## Conclusion

Congratulations! You have successfully completed a comprehensive Docker Swarm orchestration lab. You've gained hands-on experience with:

‚úÖ **Cluster Management**: Initializing and managing multi-node Docker Swarm clusters
‚úÖ **Service Orchestration**: Deploying and managing containerized applications at scale  
‚úÖ **High Availability**: Understanding automatic failover and recovery mechanisms
‚úÖ **Load Balancing**: Distributing traffic across multiple service instances
‚úÖ **Scaling Operations**: Dynamically adjusting service capacity based on demand
‚úÖ **Maintenance Procedures**: Performing updates and maintenance without downtime
‚úÖ **Troubleshooting Skills**: Diagnosing and resolving common orchestration issues

### üéØ Why This Matters for Your Career

**Immediate Impact**:
- **Production readiness**: You can now deploy and manage containerized applications in production environments
- **Operational confidence**: Understanding of how to maintain and troubleshoot orchestrated systems
- **Scalability mindset**: Appreciation for designing applications that can grow with demand

**Long-term Benefits**:
- **Foundation for Kubernetes**: Many concepts transfer directly to Kubernetes
- **Cloud-native expertise**: Skills applicable across AWS ECS, Azure Container Instances, Google Cloud Run
- **DevOps integration**: Understanding of how orchestration fits into CI/CD pipelines
- **Career advancement**: Orchestration skills are highly valued in modern infrastructure roles

The skills you've developed in this lab form the foundation for building robust, scalable, and maintainable containerized applications that can serve millions of users while maintaining high availability and performance standards.

Keep practicing, keep exploring, and remember: the best way to master orchestration is through hands-on experience with real applications and real challenges!
