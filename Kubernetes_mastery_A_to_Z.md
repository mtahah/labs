# üéì Kubernetes Mastery Lab: From Basics to Advanced Architecture

> üí° Transform from containerization novice to Kubernetes expert | ‚è±Ô∏è 12 weeks (84+ hours) | üéØ Beginner to Advanced | üíª Linux VM required

## üéØ What You'll Master

By completing this lab, you will:
- **Understand the architecture** from bare metal to production cluster
- **Experience the 7 "aha moments"** that transform understanding
- **Master debugging** the 22 most common Kubernetes errors
- **Implement production patterns** including sidecar, ambassador, and adapter
- **Build security expertise** from network policies to RBAC
- **Deploy real applications** with monitoring, autoscaling, and GitOps
- **Pass CKA certification** with troubleshooting and architecture mastery

---

## üìã Prerequisites

**System Requirements:**
- Ubuntu 22.04+ VM (4 CPU, 8GB RAM minimum, 16GB recommended)
- Root/sudo access
- Internet connectivity
- 50GB free disk space

**Verify readiness:**
```bash
# Check resources
lscpu | grep "^CPU(s)"  # Should show 4+
free -h | grep "^Mem"   # Should show 8G+
df -h /                 # Should show 50G+ available

# Check connectivity
ping -c 3 8.8.8.8

# Create report directory for state tracking
mkdir -p ~/k8s-lab/report
cd ~/k8s-lab
```

Expected: All commands succeed, resources meet minimums.

---

# PHASE 1: FOUNDATIONS (WEEKS 1-2)

## üèóÔ∏è Lab 1.1: Kubernetes The Hard Way - Understanding The Magic

**Goal:** Experience all 7 "aha moments" by manually installing every component.

### Why Manual Installation Matters

Most tutorials use `kubeadm` or managed services that hide complexity. This creates the "tutorial-production gap" where everything works until it doesn't. Manual installation reveals:
- How components actually communicate (via mutual TLS)
- Why certificate management matters (everything authenticates with certs)
- What the API server really does (central hub for all communication)
- Why networking is complex (CNI plugins, routing, DNS)

### Step 1: Certificate Infrastructure Setup

**Aha Moment #2: "Everything is certificates"**

Create the PKI infrastructure that enables mutual TLS between all components.

```bash
# Install cfssl for certificate generation
wget -q --show-progress --https-only --timestamping \
  https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \
  https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64

chmod +x cfssl_linux-amd64 cfssljson_linux-amd64
sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl
sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

# Verify
cfssl version
```

Expected: `Version: 1.2.0`

**Create CA certificate:**
```bash
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

Verify: `ls ca*.pem` ‚Üí shows `ca-key.pem` and `ca.pem`

**Critical Understanding:** Every component will authenticate using certificates signed by this CA. Without valid certs, components cannot communicate. This is why certificate expiration causes cluster failures.

**Generate component certificates:**
```bash
# Admin client certificate
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:masters",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

# Kubelet client certificate (simulating one worker)
WORKER_NAME=$(hostname -s)

cat > ${WORKER_NAME}-csr.json <<EOF
{
  "CN": "system:node:${WORKER_NAME}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKER_NAME} \
  -profile=kubernetes \
  ${WORKER_NAME}-csr.json | cfssljson -bare ${WORKER_NAME}

# Controller Manager client certificate
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

# Kube Proxy client certificate
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:node-proxier",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy

# Scheduler client certificate
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler

# Kubernetes API Server certificate
KUBERNETES_PUBLIC_ADDRESS=$(hostname -I | awk '{print $1}')

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.32.0.1,${KUBERNETES_PUBLIC_ADDRESS},127.0.0.1,kubernetes.default \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes

# Service Account key pair
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes Lab",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
```

**State Capture:**
```bash
ls -lh *.pem > ~/k8s-lab/report/certificates-created.txt
echo "Certificate infrastructure created on $(date)" >> ~/k8s-lab/report/certificates-created.txt
```

### Step 2: Generate Kubernetes Configuration Files

**Aha Moment #3: "The API server is the center of everything"**

Each component needs a kubeconfig file to locate and authenticate with the API server.

```bash
KUBERNETES_PUBLIC_ADDRESS=$(hostname -I | awk '{print $1}')
WORKER_NAME=$(hostname -s)

# Install kubectl
curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

# kubelet kubeconfig for worker
kubectl config set-cluster kubernetes-lab \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
  --kubeconfig=${WORKER_NAME}.kubeconfig

kubectl config set-credentials system:node:${WORKER_NAME} \
  --client-certificate=${WORKER_NAME}.pem \
  --client-key=${WORKER_NAME}-key.pem \
  --embed-certs=true \
  --kubeconfig=${WORKER_NAME}.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-lab \
  --user=system:node:${WORKER_NAME} \
  --kubeconfig=${WORKER_NAME}.kubeconfig

kubectl config use-context default --kubeconfig=${WORKER_NAME}.kubeconfig

# kube-proxy kubeconfig
kubectl config set-cluster kubernetes-lab \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-lab \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# kube-controller-manager kubeconfig
kubectl config set-cluster kubernetes-lab \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-lab \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig

# kube-scheduler kubeconfig
kubectl config set-cluster kubernetes-lab \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-lab \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig

# admin kubeconfig
kubectl config set-cluster kubernetes-lab \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-lab \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig
```

Verify: `ls *.kubeconfig` ‚Üí shows all component kubeconfig files

**Critical Understanding:** Controller-manager and scheduler connect to `127.0.0.1:6443` (localhost) while workers connect to the public IP because control plane components run on the same machine as the API server in this single-node setup.

### Step 3: Generate Data Encryption Config

```bash
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

**Critical Understanding:** This encrypts Secrets at rest in etcd. Without this, anyone with etcd access can read all secrets in plaintext.

### Step 4: Install etcd

**Aha Moment #4: "etcd is the brain"**

```bash
# Download etcd
wget -q --show-progress --https-only --timestamping \
  "https://github.com/etcd-io/etcd/releases/download/v3.5.9/etcd-v3.5.9-linux-amd64.tar.gz"

tar -xvf etcd-v3.5.9-linux-amd64.tar.gz
sudo mv etcd-v3.5.9-linux-amd64/etcd* /usr/local/bin/

# Configure etcd
sudo mkdir -p /etc/etcd /var/lib/etcd
sudo chmod 700 /var/lib/etcd
sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/

INTERNAL_IP=$(hostname -I | awk '{print $1}')
ETCD_NAME=$(hostname -s)

cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster ${ETCD_NAME}=https://${INTERNAL_IP}:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Start etcd
sudo systemctl daemon-reload
sudo systemctl enable etcd
sudo systemctl start etcd

# Wait for etcd to be ready
sleep 10
```

**Verify etcd is running:**
```bash
sudo ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem
```

Expected: Shows one member with status "started"

**State Capture:**
```bash
sudo systemctl status etcd --no-pager > ~/k8s-lab/report/etcd-status.txt
```

**Critical Understanding:** Kubernetes has NO state of its own‚Äîeverything lives in etcd. If etcd dies, the cluster loses all knowledge of what should be running. This is why etcd backup/restore is critical.

### Step 5: Install Control Plane Components

**Download Kubernetes binaries:**
```bash
wget -q --show-progress --https-only --timestamping \
  "https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-scheduler"

chmod +x kube-apiserver kube-controller-manager kube-scheduler
sudo mv kube-apiserver kube-controller-manager kube-scheduler /usr/local/bin/
```

**Configure API Server:**
```bash
sudo mkdir -p /var/lib/kubernetes/

sudo cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
  service-account-key.pem service-account.pem \
  encryption-config.yaml /var/lib/kubernetes/

INTERNAL_IP=$(hostname -I | awk '{print $1}')

cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=1 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://127.0.0.1:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --runtime-config='api/all=true' \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-account-issuer=https://${INTERNAL_IP}:6443 \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

**Configure Controller Manager:**
```bash
sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/

cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

**Aha Moment #6: "Controllers make desired equal actual"**

The controller-manager runs all the built-in controllers (Deployment, ReplicaSet, etc.) that continuously reconcile actual state toward desired state.

**Configure Scheduler:**
```bash
sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/
sudo mkdir -p /etc/kubernetes/config

cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

**Start all control plane services:**
```bash
sudo systemctl daemon-reload
sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler

# Wait for API server to be ready (takes ~30 seconds)
until curl --silent --cacert ca.pem https://127.0.0.1:6443/version > /dev/null 2>&1; do
  echo "Waiting for API server..."
  sleep 5
done
echo "API server is ready!"
```

**Verify all components:**
```bash
kubectl get componentstatuses --kubeconfig admin.kubeconfig
```

Expected output shows controller-manager, scheduler, and etcd all "Healthy"

**State Capture:**
```bash
kubectl get componentstatuses --kubeconfig admin.kubeconfig > ~/k8s-lab/report/control-plane-status.txt
sudo systemctl status kube-apiserver --no-pager >> ~/k8s-lab/report/control-plane-status.txt
```

### Step 6: Configure RBAC for Kubelet

```bash
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF

cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```

### Step 7: Install Worker Node Components

**Install container runtime (containerd):**
```bash
# Install dependencies
sudo apt-get update
sudo apt-get -y install socat conntrack ipset

# Download containerd
wget https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz
sudo tar -C /usr/local -xzf containerd-1.7.2-linux-amd64.tar.gz

# Configure containerd
sudo mkdir -p /etc/containerd/
cat << EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins."io.containerd.grpc.v1.cri".containerd]
    snapshotter = "overlayfs"
    [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
EOF

# Create containerd systemd service
cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF

# Install runc
wget https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64
sudo install -m 755 runc.amd64 /usr/local/bin/runc

# Install CNI plugins
sudo mkdir -p /opt/cni/bin
wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
sudo tar -C /opt/cni/bin -xzf cni-plugins-linux-amd64-v1.3.0.tgz

# Install kubelet and kube-proxy
wget -q --show-progress --https-only --timestamping \
  https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.28.0/bin/linux/amd64/kubelet

chmod +x kube-proxy kubelet
sudo mv kube-proxy kubelet /usr/local/bin/
```

**Configure kubelet:**
```bash
POD_CIDR=10.200.0.0/24
WORKER_NAME=$(hostname -s)

sudo mkdir -p /var/lib/kubelet /var/lib/kube-proxy /var/lib/kubernetes /var/run/kubernetes

# Copy certificates
sudo cp ${WORKER_NAME}-key.pem ${WORKER_NAME}.pem /var/lib/kubelet/
sudo cp ${WORKER_NAME}.kubeconfig /var/lib/kubelet/kubeconfig
sudo cp ca.pem /var/lib/kubernetes/

cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${WORKER_NAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${WORKER_NAME}-key.pem"
EOF

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

**Configure kube-proxy:**
```bash
sudo cp kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

**Start worker services:**
```bash
sudo systemctl daemon-reload
sudo systemctl enable containerd kubelet kube-proxy
sudo systemctl start containerd kubelet kube-proxy

# Wait for services to start
sleep 10
```

**Verify worker is registered:**
```bash
kubectl get nodes --kubeconfig admin.kubeconfig
```

Expected: Shows one node in "NotReady" state (networking not configured yet)

### Step 8: Configure Pod Networking

**Aha Moment #5: "Networking is the hard part"**

```bash
# Install CNI network plugin (Weave Net for simplicity)
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml --kubeconfig admin.kubeconfig

# Wait for Weave to be ready (may take 60-90 seconds)
echo "Waiting for Weave networking to be ready..."
kubectl wait --for=condition=ready pod -l name=weave-net -n kube-system --timeout=120s --kubeconfig admin.kubeconfig

# Verify node becomes Ready
kubectl get nodes --kubeconfig admin.kubeconfig
```

Expected: Node status changes to "Ready"

**State Capture:**
```bash
kubectl get nodes -o wide --kubeconfig admin.kubeconfig > ~/k8s-lab/report/nodes-ready.txt
kubectl get pods -n kube-system --kubeconfig admin.kubeconfig > ~/k8s-lab/report/system-pods.txt
```

### Step 9: Deploy CoreDNS

```bash
kubectl apply -f https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed --kubeconfig admin.kubeconfig

# Wait for CoreDNS to be ready
echo "Waiting for CoreDNS to be ready..."
kubectl wait --for=condition=ready pod -l k8s-app=kube-dns -n kube-system --timeout=120s --kubeconfig admin.kubeconfig 2>/dev/null || echo "CoreDNS might need manual configuration"
```

### Step 10: Smoke Test

**Aha Moment #1: "Kubernetes is not magic"**

Now that you've built every component manually, let's verify everything works:

```bash
# Create a test deployment
kubectl create deployment nginx --image=nginx --kubeconfig admin.kubeconfig

# Wait for deployment
kubectl wait --for=condition=available deployment/nginx --timeout=60s --kubeconfig admin.kubeconfig

# Expose it as a service
kubectl expose deployment nginx --port 80 --type NodePort --kubeconfig admin.kubeconfig

# Get the NodePort
NODE_PORT=$(kubectl get svc nginx --kubeconfig admin.kubeconfig -o jsonpath='{.spec.ports[0].nodePort}')
echo "Service available on port: $NODE_PORT"

# Test access
curl http://localhost:${NODE_PORT}
```

Expected: HTML from nginx welcome page

**Test DNS resolution:**
```bash
kubectl run busybox --image=busybox:1.28 --rm -it --restart=Never --kubeconfig admin.kubeconfig -- nslookup kubernetes
```

Expected: Shows `kubernetes.default.svc.cluster.local` resolution

**Copy admin kubeconfig for easier use:**
```bash
mkdir -p ~/.kube
cp admin.kubeconfig ~/.kube/config
# Now you can use kubectl without --kubeconfig flag
kubectl get nodes
```

**State Capture:**
```bash
kubectl get all --all-namespaces > ~/k8s-lab/report/final-cluster-state.txt
echo "Cluster fully operational on $(date)" >> ~/k8s-lab/report/final-cluster-state.txt
```

### Lab 1.1 Reflection: The 7 Aha Moments

**‚úÖ Aha #1 - "Kubernetes is not magic":** You installed each component as a Linux service with systemd. It's just processes talking to each other via standard protocols.

**‚úÖ Aha #2 - "Everything is certificates":** Every component has a certificate. Communication fails without valid certs. Certificate expiration = cluster failure. This is why organizations need cert rotation strategies.

**‚úÖ Aha #3 - "API server is the center":** All components only talk to the API server, never to each other directly. This centralizes authentication, authorization, and audit logging.

**‚úÖ Aha #4 - "etcd is the brain":** Kubernetes is stateless‚Äîall data lives in etcd. No etcd backup = no recovery from disasters.

**‚úÖ Aha #5 - "Networking is the hard part":** Manual CNI plugin installation revealed why networking dominates troubleshooting. Pod-to-pod communication across nodes requires routing tables or overlay networks.

**‚úÖ Aha #6 - "Controllers make desired equal actual":** The controller-manager constantly reconciles actual state toward desired state. This is the reconciliation loop pattern.

**‚úÖ Aha #7 - "Pods aren't containers":** Will explore in next lab with multi-container patterns.

---

## üèóÔ∏è Lab 1.2: Container Fundamentals and Pod Architecture

**Goal:** Master the distinction between containers and Pods. Understand Pod lifecycle.

**Aha Moment #7: "Pods aren't containers"**

### Step 1: Understanding Container Basics

```bash
# Pull a container image
sudo ctr image pull docker.io/library/nginx:latest

# List images
sudo ctr images list | grep nginx

# Run a container
sudo ctr run -d docker.io/library/nginx:latest nginx-test

# List running containers
sudo ctr task list

# Stop and remove
sudo ctr task kill nginx-test
sudo ctr container delete nginx-test
```

**Critical Understanding:** Containers are isolated processes with their own filesystem, network, and process namespace. Kubernetes doesn't manage containers directly‚Äîit manages Pods.

### Step 2: Create Your First Pod

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: my-first-pod
  labels:
    app: demo
    tier: frontend
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80
EOF
```

**Watch Pod creation in real-time:**
```bash
kubectl get pods my-first-pod --watch
# Press Ctrl+C after pod shows Running
```

**Examine Pod details:**
```bash
kubectl describe pod my-first-pod
```

Key observations:
- **IP address:** Pod gets its own IP (different from Node IP)
- **Events:** Shows image pull, container creation, started
- **Status:** Pending ‚Üí ContainerCreating ‚Üí Running

### Step 3: Pod Lifecycle States

Create pods in different states to understand lifecycle:

```bash
# Running pod (already created above)

# CrashLoopBackOff pod (will fail immediately)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: crash-pod
spec:
  containers:
  - name: crash
    image: busybox
    command: ["sh", "-c", "echo 'Starting...' && sleep 2 && exit 1"]
EOF

# ImagePullBackOff pod (image doesn't exist)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: imagepull-pod
spec:
  containers:
  - name: fake
    image: nonexistent/fake-image:v999
EOF

# OOMKilled pod (insufficient memory)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: oom-pod
spec:
  containers:
  - name: memory-eater
    image: progrium/stress
    args: ["--vm", "1", "--vm-bytes", "150M"]
    resources:
      limits:
        memory: "100Mi"
EOF

# Wait a moment for states to stabilize
sleep 15
```

**Observe all states:**
```bash
kubectl get pods
```

**Diagnostic commands for each error:**
```bash
# For CrashLoopBackOff
kubectl logs crash-pod
kubectl logs crash-pod --previous  # Previous crash

# For ImagePullBackOff
kubectl describe pod imagepull-pod | grep -A 5 Events

# For OOMKilled
kubectl describe pod oom-pod | grep -i oom
```

**State Capture:**
```bash
kubectl get pods > ~/k8s-lab/report/pod-states.txt
kubectl describe pod crash-pod > ~/k8s-lab/report/crashloop-debug.txt
kubectl describe pod imagepull-pod > ~/k8s-lab/report/imagepull-debug.txt
kubectl describe pod oom-pod > ~/k8s-lab/report/oom-debug.txt
```

### Step 4: Multi-Container Pods

**Critical Understanding:** Pods can contain multiple containers sharing:
- Network namespace (localhost communication)
- Storage volumes
- Lifecycle (start/stop together)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  
  - name: log-reader
    image: busybox
    command: ["sh", "-c", "tail -f /logs/access.log || sleep 3600"]
    volumeMounts:
    - name: shared-logs
      mountPath: /logs
  
  volumes:
  - name: shared-logs
    emptyDir: {}
EOF

# Wait for pod to be ready
kubectl wait --for=condition=ready pod/multi-container-pod --timeout=60s
```

**Test localhost communication:**
```bash
# Exec into the log-reader container
kubectl exec -it multi-container-pod -c log-reader -- sh

# Inside the container, nginx is accessible via localhost:
# wget -O- http://localhost
# exit

# Or test it directly:
kubectl exec multi-container-pod -c log-reader -- wget -O- http://localhost
```

**Key Insight:** Containers in the same Pod share a network namespace‚Äîthey can talk via `localhost`.

### Step 5: Labels and Selectors

**Critical Understanding:** Labels are the glue that connects everything in Kubernetes.

```bash
# Create pods with different labels
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: frontend-prod
  labels:
    app: web
    tier: frontend
    environment: production
spec:
  containers:
  - name: nginx
    image: nginx
---
apiVersion: v1
kind: Pod
metadata:
  name: frontend-dev
  labels:
    app: web
    tier: frontend
    environment: development
spec:
  containers:
  - name: nginx
    image: nginx
---
apiVersion: v1
kind: Pod
metadata:
  name: backend-prod
  labels:
    app: api
    tier: backend
    environment: production
spec:
  containers:
  - name: app
    image: busybox
    command: ["sleep", "3600"]
EOF

# Wait for pods
kubectl wait --for=condition=ready pod/frontend-prod pod/frontend-dev pod/backend-prod --timeout=60s
```

**Use selectors to find pods:**
```bash
# All frontend pods
kubectl get pods -l tier=frontend

# Production pods only
kubectl get pods -l environment=production

# Frontend AND production
kubectl get pods -l 'tier=frontend,environment=production'

# NOT development
kubectl get pods -l 'environment!=development'

# Show all labels
kubectl get pods --show-labels
```

**State Capture:**
```bash
kubectl get pods --show-labels > ~/k8s-lab/report/pod-labels.txt
```

---

# PHASE 2: CORE WORKLOADS (WEEKS 3-4)

## üöÄ Lab 2.1: ReplicaSets and Deployments

**Goal:** Understand how Kubernetes ensures desired state through controllers.

### Step 1: ReplicaSet Fundamentals

```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 80
EOF

# Wait for all replicas
kubectl wait --for=condition=ready pod -l app=nginx --timeout=60s
```

**Observe self-healing:**
```bash
# Watch pods in one terminal
kubectl get pods -l app=nginx --watch &
WATCH_PID=$!

# Delete a pod
POD_TO_DELETE=$(kubectl get pods -l app=nginx -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $POD_TO_DELETE

# ReplicaSet immediately creates a replacement
sleep 5
kill $WATCH_PID 2>/dev/null
```

**Scale manually:**
```bash
kubectl scale replicaset nginx-replicaset --replicas=5
kubectl get pods -l app=nginx
```

Expected: 5 running nginx pods

### Step 2: Deployment Strategies

**Create a Deployment:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx-deploy
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: nginx-deploy
  template:
    metadata:
      labels:
        app: nginx-deploy
    spec:
      containers:
      - name: nginx
        image: nginx:1.24
        ports:
        - containerPort: 80
EOF

# Wait for deployment
kubectl wait --for=condition=available deployment/nginx-deployment --timeout=60s
```

**Perform a rolling update:**
```bash
# Update image version
kubectl set image deployment/nginx-deployment nginx=nginx:1.25

# Watch rollout
kubectl rollout status deployment/nginx-deployment

# View rollout history
kubectl rollout history deployment/nginx-deployment
```

**Rollback if needed:**
```bash
kubectl rollout undo deployment/nginx-deployment
kubectl rollout status deployment/nginx-deployment
```

**State Capture:**
```bash
kubectl rollout history deployment/nginx-deployment > ~/k8s-lab/report/deployment-history.txt
kubectl describe deployment nginx-deployment > ~/k8s-lab/report/deployment-details.txt
```

### Step 3: Blue-Green Deployment Pattern

```bash
# Blue version (current)
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: nginx
        image: nginx:1.24
        env:
        - name: VERSION
          value: "blue"
        ports:
        - containerPort: 80
EOF

# Service initially points to blue
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
    version: blue
  ports:
  - port: 80
    targetPort: 80
EOF

# Wait for blue deployment
kubectl wait --for=condition=available deployment/blue-deployment --timeout=60s

# Deploy green version (new)
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        env:
        - name: VERSION
          value: "green"
        ports:
        - containerPort: 80
EOF

# Wait for green deployment
kubectl wait --for=condition=available deployment/green-deployment --timeout=60s

# Test green version directly
GREEN_POD=$(kubectl get pods -l version=green -o jsonpath='{.items[0].metadata.name}')
kubectl port-forward $GREEN_POD 8080:80 &
PF_PID=$!
sleep 2
curl http://localhost:8080 | grep -i nginx
kill $PF_PID 2>/dev/null

# Switch traffic to green (instant cutover)
kubectl patch service myapp-service -p '{"spec":{"selector":{"version":"green"}}}'

echo "Traffic switched to green version"

# If needed, rollback to blue (instant)
# kubectl patch service myapp-service -p '{"spec":{"selector":{"version":"blue"}}}'
```

## üåê Lab 2.2: Services and Service Discovery

**Goal:** Master how Kubernetes enables service-to-service communication.

### Step 1: ClusterIP Service

```bash
# Create deployment
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
EOF

# Create ClusterIP service
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
EOF

# Wait for deployment
kubectl wait --for=condition=available deployment/web-app --timeout=60s
```

**Verify service and endpoints:**
```bash
kubectl get service web-service
kubectl get endpoints web-service
```

**Critical Understanding:** Service creates a stable IP. Endpoints lists all pod IPs matching the selector.

**Test DNS resolution:**
```bash
kubectl run test-dns --image=busybox:1.28 --rm -it --restart=Never -- nslookup web-service
```

Expected: Returns service ClusterIP

**Test connectivity:**
```bash
kubectl run test-curl --image=curlimages/curl --rm -it --restart=Never -- curl http://web-service
```

Expected: nginx welcome page

### Step 2: NodePort Service

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: web-nodeport
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
EOF
```

**Access from outside cluster:**
```bash
NODE_IP=$(hostname -I | awk '{print $1}')
curl http://${NODE_IP}:30080
```

**Critical Understanding:** NodePort opens a port on every node, routing to the service.

### Step 3: Headless Service (for StatefulSets)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: web-headless
spec:
  clusterIP: None
  selector:
    app: web
  ports:
  - port: 80
EOF
```

**Test DNS returns individual pod IPs:**
```bash
kubectl run test-headless --image=busybox:1.28 --rm -it --restart=Never -- nslookup web-headless
```

Expected: Returns all 3 pod IPs (not a single service IP)

**State Capture:**
```bash
kubectl get services > ~/k8s-lab/report/services.txt
kubectl get endpoints > ~/k8s-lab/report/endpoints.txt
```

## üîê Lab 2.3: ConfigMaps and Secrets

**Goal:** Externalize configuration from container images.

### Step 1: ConfigMaps

```bash
# Create ConfigMap from literal values
kubectl create configmap app-config \
  --from-literal=APP_ENV=production \
  --from-literal=APP_PORT=8080 \
  --from-literal=LOG_LEVEL=info

# Create ConfigMap from file
cat > app.properties <<EOF
database.host=postgres.default.svc.cluster.local
database.port=5432
database.name=appdb
cache.enabled=true
cache.ttl=300
EOF

kubectl create configmap app-properties --from-file=app.properties

# View ConfigMaps
kubectl get configmaps
kubectl describe configmap app-config
```

**Use ConfigMap as environment variables:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: config-env-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "env | grep APP_ && sleep 3600"]
    env:
    - name: APP_ENV
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_ENV
    - name: APP_PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_PORT
EOF

# Wait and verify environment variables
kubectl wait --for=condition=ready pod/config-env-pod --timeout=30s
kubectl logs config-env-pod | grep APP_
```

**Mount ConfigMap as volume:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: config-volume-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "cat /config/app.properties && sleep 3600"]
    volumeMounts:
    - name: config-volume
      mountPath: /config
  volumes:
  - name: config-volume
    configMap:
      name: app-properties
EOF

# Verify file contents
kubectl wait --for=condition=ready pod/config-volume-pod --timeout=30s
kubectl logs config-volume-pod
```

### Step 2: Secrets

```bash
# Create secret from literal
kubectl create secret generic db-credentials \
  --from-literal=username=admin \
  --from-literal=password=supersecret123

# Create secret from file
echo -n 'my-api-key-12345' > api-key.txt
kubectl create secret generic api-secret --from-file=api-key=api-key.txt
rm api-key.txt

# View secrets (values are base64 encoded)
kubectl get secrets
kubectl get secret db-credentials -o yaml
```

**Use Secret as environment variable:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo Username: \$DB_USER && echo Password: \$DB_PASS && sleep 3600"]
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: username
    - name: DB_PASS
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password
EOF

kubectl wait --for=condition=ready pod/secret-env-pod --timeout=30s
kubectl logs secret-env-pod
```

**Mount Secret as volume:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "ls -la /secrets/ && cat /secrets/api-key && sleep 3600"]
    volumeMounts:
    - name: secret-volume
      mountPath: /secrets
      readOnly: true
  volumes:
  - name: secret-volume
    secret:
      secretName: api-secret
EOF

kubectl wait --for=condition=ready pod/secret-volume-pod --timeout=30s
kubectl logs secret-volume-pod
```

**Common Error: CreateContainerConfigError**

```bash
# This will fail - ConfigMap doesn't exist
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: missing-config-pod
spec:
  containers:
  - name: app
    image: nginx
    envFrom:
    - configMapRef:
        name: nonexistent-config
EOF

# Diagnose
sleep 5
kubectl describe pod missing-config-pod | grep -A 5 Events
```

Expected error: `CreateContainerConfigError` indicating missing ConfigMap

**State Capture:**
```bash
kubectl get configmaps > ~/k8s-lab/report/configmaps.txt
kubectl get secrets > ~/k8s-lab/report/secrets-list.txt
```

**Completed**: Part 1 (Phases 1-2) - Kubernetes The Hard Way, Container Fundamentals, Core Workloads

**Verify Part 1 cluster is running**:
```bash
kubectl get nodes
kubectl get pods --all-namespaces
```

Expected: Control plane + worker nodes Ready, CoreDNS pods Running

**Create lab directory for Part 2**:
```bash
mkdir -p ~/k8s-lab-part2/report
cd ~/k8s-lab-part2
```

---

# PHASE 3: ADVANCED PATTERNS (WEEKS 5-6)

## üé® Lab 3.1: Multi-Container Design Patterns

**Goal**: Master sidecar, ambassador, and adapter patterns through real-world scenarios.

**Learning Outcomes**:
- Understand when to use each pattern
- Implement inter-container communication
- Debug multi-container pod issues

### Aha Moment #8: Containers Share Pod Resources

**Concept**: Containers in the same pod share network namespace and volumes.

**Prove it**:
```bash
# Create namespace
kubectl create namespace patterns

# Deploy sidecar pattern - web server with logging sidecar
cat > sidecar-pattern.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: web-with-sidecar
  namespace: patterns
  labels:
    pattern: sidecar
spec:
  volumes:
  - name: shared-logs
    emptyDir: {}
  
  containers:
  # Main application container
  - name: nginx
    image: nginx:1.25
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
    ports:
    - containerPort: 80
  
  # Sidecar: log processor
  - name: log-processor
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      echo "Log processor started"
      while true; do
        if [ -f /var/log/nginx/access.log ]; then
          echo "=== Access Log Stats ($(date)) ==="
          wc -l /var/log/nginx/access.log
          echo "---"
        fi
        sleep 10
      done
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
      readOnly: true
EOF

kubectl apply -f sidecar-pattern.yaml

# Wait for pod ready
kubectl wait --for=condition=ready pod/web-with-sidecar -n patterns --timeout=60s
```

**Verify sidecar can access main container's logs**:
```bash
# Generate traffic to nginx
kubectl exec -n patterns web-with-sidecar -c nginx -- curl localhost

# Check sidecar processed the log
kubectl logs -n patterns web-with-sidecar -c log-processor --tail=20
```

Expected: Sidecar shows "Access Log Stats" with line count

**Capture state**:
```bash
kubectl get pod web-with-sidecar -n patterns -o yaml > ~/k8s-lab-part2/report/sidecar-state.yaml
echo "Sidecar pattern verified: $(date)" >> ~/k8s-lab-part2/report/patterns.log
```

---

### Ambassador Pattern: External Service Proxy

**Use case**: Single pod connects to multiple external services through local proxy.

```bash
cat > ambassador-pattern.yaml <<'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: haproxy-config
  namespace: patterns
data:
  haproxy.cfg: |
    global
      maxconn 256
    
    defaults
      mode http
      timeout connect 5000ms
      timeout client 50000ms
      timeout server 50000ms
    
    frontend http-in
      bind *:8080
      default_backend servers
    
    backend servers
      server api1 httpbin.org:80 check
---
apiVersion: v1
kind: Pod
metadata:
  name: app-with-ambassador
  namespace: patterns
  labels:
    pattern: ambassador
spec:
  containers:
  # Main application
  - name: app
    image: curlimages/curl:8.5.0
    command: ['sh', '-c']
    args:
    - |
      echo "App started - using ambassador at localhost:8080"
      while true; do
        echo "Request via ambassador:"
        curl -s localhost:8080/headers | head -5
        sleep 30
      done
  
  # Ambassador: proxy to external services
  - name: ambassador
    image: haproxy:2.9-alpine
    volumeMounts:
    - name: config
      mountPath: /usr/local/etc/haproxy/haproxy.cfg
      subPath: haproxy.cfg
  
  volumes:
  - name: config
    configMap:
      name: haproxy-config
EOF

kubectl apply -f ambassador-pattern.yaml
kubectl wait --for=condition=ready pod/app-with-ambassador -n patterns --timeout=60s

# Verify ambassador proxying
kubectl logs -n patterns app-with-ambassador -c app --tail=10
```

Expected: App successfully makes requests through localhost:8080 (ambassador)

---

### Adapter Pattern: Standardize Output

**Use case**: Legacy app outputs non-standard format; adapter normalizes it.

```bash
cat > adapter-pattern.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: legacy-with-adapter
  namespace: patterns
  labels:
    pattern: adapter
spec:
  volumes:
  - name: shared-data
    emptyDir: {}
  
  containers:
  # Legacy application (outputs custom format)
  - name: legacy-app
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      while true; do
        echo "LEGACY_LOG|$(date)|USER=admin|ACTION=login|STATUS=success" >> /data/app.log
        sleep 5
      done
    volumeMounts:
    - name: shared-data
      mountPath: /data
  
  # Adapter: converts to JSON format
  - name: format-adapter
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      echo "Adapter started - converting legacy format to JSON"
      tail -f /data/app.log | while read line; do
        timestamp=$(echo $line | cut -d'|' -f2)
        user=$(echo $line | cut -d'|' -f3 | cut -d'=' -f2)
        action=$(echo $line | cut -d'|' -f4 | cut -d'=' -f2)
        status=$(echo $line | cut -d'|' -f5 | cut -d'=' -f2)
        echo "{\"timestamp\":\"$timestamp\",\"user\":\"$user\",\"action\":\"$action\",\"status\":\"$status\"}"
      done
    volumeMounts:
    - name: shared-data
      mountPath: /data
      readOnly: true
EOF

kubectl apply -f adapter-pattern.yaml
kubectl wait --for=condition=ready pod/legacy-with-adapter -n patterns --timeout=60s

# Compare outputs
echo "=== Legacy Format ==="
kubectl logs -n patterns legacy-with-adapter -c legacy-app --tail=3

echo -e "\n=== Adapted JSON Format ==="
kubectl logs -n patterns legacy-with-adapter -c format-adapter --tail=3
```

Expected: Adapter outputs valid JSON from legacy pipe-delimited format

**Pattern comparison**:
```bash
kubectl get pods -n patterns -o custom-columns=\
NAME:.metadata.name,\
PATTERN:.metadata.labels.pattern,\
CONTAINERS:.spec.containers[*].name
```

**Capture pattern evidence**:
```bash
cat > ~/k8s-lab-part2/report/patterns-summary.txt <<EOF
Multi-Container Patterns Verified: $(date)

1. Sidecar: web-with-sidecar - log processor accesses nginx logs via shared volume
2. Ambassador: app-with-ambassador - app uses localhost proxy to external services
3. Adapter: legacy-with-adapter - converts legacy format to JSON

Key Insight: All patterns use pod's shared network (localhost) and volumes (emptyDir)
EOF
```

---

## üè• Lab 3.2: Health Probes & Lifecycle Management

**Goal**: Implement liveness, readiness, and startup probes to ensure pod reliability.

### Aha Moment #9: Kubernetes Can't Read Your App's Mind

**Concept**: Running ‚â† Ready. Probes tell Kubernetes your app's true state.

### Liveness Probe: Detecting Deadlocks

```bash
cat > liveness-probe.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: liveness-test
  namespace: patterns
spec:
  containers:
  - name: app
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      touch /tmp/healthy
      echo "App started, healthy file created"
      
      # Simulate app becoming unhealthy after 30 seconds
      sleep 30
      rm /tmp/healthy
      echo "App became unhealthy - file removed"
      
      # Keep container running
      while true; do sleep 10; done
    
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 2
EOF

kubectl apply -f liveness-probe.yaml

# Monitor pod - should restart after ~40 seconds
watch -n 2 "kubectl get pod liveness-test -n patterns"
```

**Wait for restart** (in another terminal or after watch):
```bash
# Check events to see liveness failure
kubectl describe pod liveness-test -n patterns | grep -A 5 "Events:"

# Verify restart count increased
kubectl get pod liveness-test -n patterns -o jsonpath='{.status.containerStatuses[0].restartCount}'
```

Expected: Pod restarts after liveness probe fails (RESTARTS > 0)

---

### Readiness Probe: Traffic Control

```bash
cat > readiness-probe.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: readiness-test
  namespace: patterns
  labels:
    app: ready-app
spec:
  containers:
  - name: app
    image: nginx:1.25
    readinessProbe:
      httpGet:
        path: /health
        port: 80
      initialDelaySeconds: 3
      periodSeconds: 5
    lifecycle:
      postStart:
        exec:
          command:
          - sh
          - -c
          - |
            # Create health endpoint after 10 seconds
            sleep 10
            echo "OK" > /usr/share/nginx/html/health
---
apiVersion: v1
kind: Service
metadata:
  name: ready-svc
  namespace: patterns
spec:
  selector:
    app: ready-app
  ports:
  - port: 80
    targetPort: 80
EOF

kubectl apply -f readiness-probe.yaml

# Watch readiness status change
kubectl get pod readiness-test -n patterns -w
```

**Verify endpoints updated**:
```bash
# Before ready: no endpoints
kubectl get endpoints ready-svc -n patterns

# After ~13 seconds: endpoint appears
sleep 15
kubectl get endpoints ready-svc -n patterns -o jsonpath='{.subsets[*].addresses[*].ip}'
```

Expected: Pod stays in `0/1 Ready` state, then transitions to `1/1 Ready` after health endpoint created

---

### Startup Probe: Slow-Starting Apps

```bash
cat > startup-probe.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: slow-starter
  namespace: patterns
spec:
  containers:
  - name: app
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      echo "App initializing (slow startup)..."
      sleep 45
      touch /tmp/started
      echo "App fully started"
      while true; do sleep 10; done
    
    startupProbe:
      exec:
        command:
        - cat
        - /tmp/started
      periodSeconds: 5
      failureThreshold: 15  # 15 * 5s = 75s max startup time
    
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/started
      periodSeconds: 5
      # Liveness only starts checking AFTER startup succeeds
EOF

kubectl apply -f startup-probe.yaml

# Monitor startup process
kubectl get pod slow-starter -n patterns --watch
```

**Verify probe sequence**:
```bash
kubectl describe pod slow-starter -n patterns | grep -E "Started|Liveness|Startup"
```

Expected: Startup probe checks first (fails initially), then succeeds, then liveness probe begins

---

### PreStop Hook: Graceful Shutdown

```bash
cat > graceful-shutdown.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: graceful-app
  namespace: patterns
spec:
  containers:
  - name: app
    image: nginx:1.25
    lifecycle:
      preStop:
        exec:
          command:
          - sh
          - -c
          - |
            echo "Received termination signal at $(date)" >> /usr/share/nginx/html/shutdown.log
            echo "Draining connections..."
            sleep 10
            echo "Shutdown complete" >> /usr/share/nginx/html/shutdown.log
  terminationGracePeriodSeconds: 30
EOF

kubectl apply -f graceful-shutdown.yaml
kubectl wait --for=condition=ready pod/graceful-app -n patterns --timeout=30s

# Trigger graceful shutdown
kubectl delete pod graceful-app -n patterns &

# Quickly check logs during shutdown
sleep 2
kubectl logs graceful-app -n patterns --previous 2>/dev/null || echo "Pod still running"
```

**Capture probe configurations**:
```bash
cat > ~/k8s-lab-part2/report/probe-configs.txt <<EOF
Health Probe Summary: $(date)

Liveness Probe:
- Purpose: Restart deadlocked containers
- Test: liveness-test pod restarted after /tmp/healthy removed
- Restart count: $(kubectl get pod liveness-test -n patterns -o jsonpath='{.status.containerStatuses[0].restartCount}' 2>/dev/null || echo "N/A")

Readiness Probe:
- Purpose: Control traffic routing
- Test: readiness-test not added to endpoints until /health exists
- Endpoint IP: $(kubectl get endpoints ready-svc -n patterns -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || echo "N/A")

Startup Probe:
- Purpose: Protect slow-starting apps from liveness checks
- Test: slow-starter given 75s to start before liveness begins
- Status: $(kubectl get pod slow-starter -n patterns -o jsonpath='{.status.phase}' 2>/dev/null || echo "N/A")

PreStop Hook:
- Purpose: Graceful shutdown with cleanup
- Test: graceful-app executes 10s drain before termination
EOF
```

---

## üìä Lab 3.3: Resource Management & QoS

**Goal**: Control CPU/memory allocation and understand Quality of Service classes.

### Resource Requests vs Limits

```bash
cat > resource-demo.yaml <<'EOF'
apiVersion: v1
kind: Namespace
metadata:
  name: resources
---
# Pod 1: Guaranteed QoS (requests = limits)
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
  namespace: resources
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        memory: "128Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "250m"
---
# Pod 2: Burstable QoS (requests < limits)
apiVersion: v1
kind: Pod
metadata:
  name: burstable-pod
  namespace: resources
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "500m"
---
# Pod 3: BestEffort QoS (no requests/limits)
apiVersion: v1
kind: Pod
metadata:
  name: besteffort-pod
  namespace: resources
spec:
  containers:
  - name: app
    image: nginx:1.25
EOF

kubectl apply -f resource-demo.yaml
kubectl wait --for=condition=ready pod --all -n resources --timeout=60s

# Show QoS classes
kubectl get pods -n resources -o custom-columns=\
NAME:.metadata.name,\
QOS:.status.qosClass,\
CPU_REQ:.spec.containers[0].resources.requests.cpu,\
CPU_LIM:.spec.containers[0].resources.limits.cpu,\
MEM_REQ:.spec.containers[0].resources.requests.memory,\
MEM_LIM:.spec.containers[0].resources.limits.memory
```

**Expected QoS assignment**:
- `guaranteed-pod`: Guaranteed (requests = limits)
- `burstable-pod`: Burstable (requests exist, < limits)
- `besteffort-pod`: BestEffort (no requests/limits)

---

### Memory Pressure Test: Eviction Order

**Concept**: Under memory pressure, Kubernetes evicts BestEffort ‚Üí Burstable ‚Üí Guaranteed.

```bash
# Create memory-intensive pod
cat > memory-hog.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: memory-hog
  namespace: resources
spec:
  containers:
  - name: stress
    image: polinux/stress:1.0.4
    command: ["stress"]
    args:
    - "--vm"
    - "1"
    - "--vm-bytes"
    - "512M"
    - "--vm-hang"
    - "0"
    resources:
      requests:
        memory: "256Mi"
      limits:
        memory: "600Mi"
EOF

kubectl apply -f memory-hog.yaml

# Monitor pod status and events
kubectl get pods -n resources --watch
# In another terminal:
kubectl describe pod memory-hog -n resources | tail -20
```

**If pod gets OOMKilled**:
```bash
kubectl get pod memory-hog -n resources -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'
```

Expected: Shows "OOMKilled" if memory limit exceeded

---

### LimitRange: Namespace-Level Defaults

```bash
cat > limitrange-demo.yaml <<'EOF'
apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
  namespace: resources
spec:
  limits:
  - max:
      memory: "1Gi"
      cpu: "1"
    min:
      memory: "64Mi"
      cpu: "100m"
    default:
      memory: "256Mi"
      cpu: "500m"
    defaultRequest:
      memory: "128Mi"
      cpu: "250m"
    type: Container
EOF

kubectl apply -f limitrange-demo.yaml

# Test: create pod without resources specified
cat > auto-resources-pod.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: auto-resources
  namespace: resources
spec:
  containers:
  - name: app
    image: nginx:1.25
EOF

kubectl apply -f auto-resources-pod.yaml

# Verify defaults were applied
kubectl get pod auto-resources -n resources -o jsonpath='{.spec.containers[0].resources}'
```

Expected: Shows default requests/limits from LimitRange

---

### ResourceQuota: Namespace-Level Caps

```bash
cat > resourcequota-demo.yaml <<'EOF'
apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: resources
spec:
  hard:
    requests.cpu: "2"
    requests.memory: "2Gi"
    limits.cpu: "4"
    limits.memory: "4Gi"
    pods: "10"
EOF

kubectl apply -f resourcequota-demo.yaml

# Check quota status
kubectl get resourcequota namespace-quota -n resources -o yaml
kubectl describe resourcequota namespace-quota -n resources
```

**Test quota enforcement**:
```bash
# Try to exceed pod limit (currently 5 pods exist)
for i in {1..6}; do
  kubectl run quota-test-$i --image=nginx:1.25 -n resources --requests=cpu=100m,memory=64Mi
done

# Check if last pods were rejected
kubectl get pods -n resources | grep quota-test
```

Expected: Some pods fail to create with quota exceeded error

**Capture resource state**:
```bash
cat > ~/k8s-lab-part2/report/resource-summary.txt <<EOF
Resource Management Summary: $(date)

QoS Classes:
$(kubectl get pods -n resources -o custom-columns=NAME:.metadata.name,QOS:.status.qosClass --no-headers)

LimitRange Defaults Applied:
$(kubectl get pod auto-resources -n resources -o jsonpath='{.spec.containers[0].resources}')

ResourceQuota Usage:
$(kubectl describe resourcequota namespace-quota -n resources | grep -A 8 "Used")

Key Learning: QoS determines eviction priority under pressure (BestEffort evicted first)
EOF
```

---

## üß™ Lab 3.4: Testing & Validation

**Verify all Phase 3 concepts**:
```bash
cat > ~/k8s-lab-part2/phase3-test.sh <<'EOF'
#!/bin/bash
set -e

echo "=== Phase 3 Validation ==="

# Test 1: Multi-container patterns
echo "1. Testing patterns..."
kubectl get pod web-with-sidecar -n patterns -o jsonpath='{.spec.containers[*].name}' | grep -q "nginx log-processor" && echo "‚úÖ Sidecar pattern" || echo "‚ùå Sidecar failed"
kubectl get pod app-with-ambassador -n patterns -o jsonpath='{.spec.containers[*].name}' | grep -q "app ambassador" && echo "‚úÖ Ambassador pattern" || echo "‚ùå Ambassador failed"
kubectl get pod legacy-with-adapter -n patterns -o jsonpath='{.spec.containers[*].name}' | grep -q "legacy-app format-adapter" && echo "‚úÖ Adapter pattern" || echo "‚ùå Adapter failed"

# Test 2: Health probes
echo -e "\n2. Testing health probes..."
restart_count=$(kubectl get pod liveness-test -n patterns -o jsonpath='{.status.containerStatuses[0].restartCount}')
[ "$restart_count" -gt 0 ] && echo "‚úÖ Liveness probe triggered restart" || echo "‚ùå Liveness not triggered"

ready_status=$(kubectl get pod readiness-test -n patterns -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
[ "$ready_status" == "True" ] && echo "‚úÖ Readiness probe passed" || echo "‚ùå Readiness not ready"

# Test 3: Resource management
echo -e "\n3. Testing QoS classes..."
qos_guaranteed=$(kubectl get pod guaranteed-pod -n resources -o jsonpath='{.status.qosClass}')
qos_burstable=$(kubectl get pod burstable-pod -n resources -o jsonpath='{.status.qosClass}')
qos_besteffort=$(kubectl get pod besteffort-pod -n resources -o jsonpath='{.status.qosClass}')

[ "$qos_guaranteed" == "Guaranteed" ] && echo "‚úÖ Guaranteed QoS" || echo "‚ùå QoS mismatch"
[ "$qos_burstable" == "Burstable" ] && echo "‚úÖ Burstable QoS" || echo "‚ùå QoS mismatch"
[ "$qos_besteffort" == "BestEffort" ] && echo "‚úÖ BestEffort QoS" || echo "‚ùå QoS mismatch"

echo -e "\n=== Phase 3 Complete ==="
EOF

chmod +x ~/k8s-lab-part2/phase3-test.sh
~/k8s-lab-part2/phase3-test.sh > ~/k8s-lab-part2/report/phase3-results.txt 2>&1
cat ~/k8s-lab-part2/report/phase3-results.txt
```

---

# PHASE 4: STORAGE & NETWORKING (WEEKS 7-8)

## üíæ Lab 4.1: Persistent Storage

**Goal**: Understand volume types and implement persistent data storage.

### Aha Moment #10: Pods Are Ephemeral, Data Shouldn't Be

**Concept**: Volumes outlive containers; PersistentVolumes outlive pods.

### Volume Types Comparison

```bash
kubectl create namespace storage

# 1. emptyDir: Temporary storage (lost when pod deleted)
cat > emptydir-demo.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-pod
  namespace: storage
spec:
  containers:
  - name: writer
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      echo "Writing to emptyDir at $(date)" > /data/timestamp.txt
      cat /data/timestamp.txt
      sleep 3600
    volumeMounts:
    - name: temp-storage
      mountPath: /data
  volumes:
  - name: temp-storage
    emptyDir: {}
EOF

kubectl apply -f emptydir-demo.yaml
kubectl wait --for=condition=ready pod/emptydir-pod -n storage --timeout=60s

# Read data
kubectl exec -n storage emptydir-pod -- cat /data/timestamp.txt

# Delete and recreate - data is lost
kubectl delete pod emptydir-pod -n storage
kubectl apply -f emptydir-demo.yaml
kubectl wait --for=condition=ready pod/emptydir-pod -n storage --timeout=60s
kubectl exec -n storage emptydir-pod -- cat /data/timestamp.txt
```

Expected: New timestamp (data was lost)

---

### HostPath: Node-Level Storage

```bash
cat > hostpath-demo.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
  namespace: storage
spec:
  containers:
  - name: app
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      echo "HostPath data persists: $(date)" > /host-data/persistent.txt
      cat /host-data/persistent.txt
      sleep 3600
    volumeMounts:
    - name: host-storage
      mountPath: /host-data
  volumes:
  - name: host-storage
    hostPath:
      path: /tmp/k8s-hostpath-demo
      type: DirectoryOrCreate
EOF

kubectl apply -f hostpath-demo.yaml
kubectl wait --for=condition=ready pod/hostpath-pod -n storage --timeout=60s

# Verify data persists after pod recreation
kubectl delete pod hostpath-pod -n storage
kubectl apply -f hostpath-demo.yaml
kubectl wait --for=condition=ready pod/hostpath-pod -n storage --timeout=60s
kubectl exec -n storage hostpath-pod -- cat /host-data/persistent.txt
```

Expected: Shows original timestamp (data persisted on node)

---

### PersistentVolume & PersistentVolumeClaim

```bash
# Create PersistentVolume (admin creates)
cat > pv-demo.yaml <<'EOF'
apiVersion: v1
kind: PersistentVolume
metadata:
  name: demo-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /tmp/k8s-pv-demo
    type: DirectoryOrCreate
EOF

kubectl apply -f pv-demo.yaml

# Verify PV created
kubectl get pv demo-pv
```

Expected: STATUS shows "Available"

```bash
# Create PersistentVolumeClaim (user requests storage)
cat > pvc-demo.yaml <<'EOF'
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: demo-pvc
  namespace: storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  storageClassName: manual
EOF

kubectl apply -f pvc-demo.yaml

# Verify PVC bound to PV
kubectl get pvc demo-pvc -n storage
kubectl get pv demo-pv
```

Expected: PVC STATUS "Bound", PV STATUS "Bound"

```bash
# Use PVC in pod
cat > pvc-pod.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: pvc-pod
  namespace: storage
spec:
  containers:
  - name: app
    image: nginx:1.25
    volumeMounts:
    - name: persistent-storage
      mountPath: /usr/share/nginx/html
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: demo-pvc
EOF

kubectl apply -f pvc-pod.yaml
kubectl wait --for=condition=ready pod/pvc-pod -n storage --timeout=60s

# Write data
kubectl exec -n storage pvc-pod -- sh -c "echo 'PVC data persists' > /usr/share/nginx/html/index.html"

# Delete pod, create new one - data persists
kubectl delete pod pvc-pod -n storage
kubectl apply -f pvc-pod.yaml
kubectl wait --for=condition=ready pod/pvc-pod -n storage --timeout=60s
kubectl exec -n storage pvc-pod -- cat /usr/share/nginx/html/index.html
```

Expected: Original "PVC data persists" message

---

### StatefulSet with PVC Template

```bash
cat > statefulset-storage.yaml <<'EOF'
apiVersion: v1
kind: Service
metadata:
  name: stateful-svc
  namespace: storage
spec:
  clusterIP: None
  selector:
    app: stateful
  ports:
  - port: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: stateful-demo
  namespace: storage
spec:
  serviceName: stateful-svc
  replicas: 2
  selector:
    matchLabels:
      app: stateful
  template:
    metadata:
      labels:
        app: stateful
    spec:
      containers:
      - name: app
        image: nginx:1.25
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: manual
      resources:
        requests:
          storage: 100Mi
EOF

# Create PVs for StatefulSet (need 2)
cat > statefulset-pvs.yaml <<'EOF'
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-0
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /tmp/k8s-pv-0
    type: DirectoryOrCreate
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-1
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /tmp/k8s-pv-1
    type: DirectoryOrCreate
EOF

kubectl apply -f statefulset-pvs.yaml
kubectl apply -f statefulset-storage.yaml

# Wait for StatefulSet ready
kubectl rollout status statefulset/stateful-demo -n storage --timeout=120s

# Verify each pod has unique PVC
kubectl get pvc -n storage
kubectl get pods -n storage -l app=stateful -o custom-columns=\
NAME:.metadata.name,\
PVC:.spec.volumes[0].persistentVolumeClaim.claimName
```

**Write unique data to each pod**:
```bash
kubectl exec -n storage stateful-demo-0 -- sh -c "echo 'Pod 0 data' > /data/id.txt"
kubectl exec -n storage stateful-demo-1 -- sh -c "echo 'Pod 1 data' > /data/id.txt"

# Delete StatefulSet (keep PVCs)
kubectl delete statefulset stateful-demo -n storage

# Recreate - data persists
kubectl apply -f statefulset-storage.yaml
kubectl rollout status statefulset/stateful-demo -n storage --timeout=120s

# Verify data survived
kubectl exec -n storage stateful-demo-0 -- cat /data/id.txt
kubectl exec -n storage stateful-demo-1 -- cat /data/id.txt
```

Expected: Each pod shows its original unique data

**Capture storage state**:
```bash
cat > ~/k8s-lab-part2/report/storage-summary.txt <<EOF
Persistent Storage Summary: $(date)

Volume Types Tested:
1. emptyDir: $(kubectl get pod emptydir-pod -n storage -o jsonpath='{.spec.volumes[0].emptyDir}' 2>/dev/null || echo "N/A")
2. hostPath: $(kubectl get pod hostpath-pod -n storage -o jsonpath='{.spec.volumes[0].hostPath.path}' 2>/dev/null || echo "N/A")
3. PV/PVC: $(kubectl get pvc demo-pvc -n storage -o jsonpath='{.status.phase}' 2>/dev/null || echo "N/A")

StatefulSet PVCs:
$(kubectl get pvc -n storage -l app=stateful --no-headers)

Key Learning: emptyDir < hostPath < PV/PVC in terms of data persistence scope
EOF
```

---

## üåê Lab 4.2: Networking Deep Dive

**Goal**: Master Kubernetes networking from pod communication to ingress.

### Aha Moment #11: Every Pod Gets an IP

**Concept**: Kubernetes flat network - all pods can talk to all pods without NAT.

```bash
kubectl create namespace network

# Deploy test pods across namespaces
cat > network-test.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  namespace: network
  labels:
    app: test-app
spec:
  containers:
  - name: web
    image: nginx:1.25
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-b
  namespace: network
spec:
  containers:
  - name: client
    image: curlimages/curl:8.5.0
    command: ['sleep', '3600']
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-c
  namespace: default
spec:
  containers:
  - name: client
    image: curlimages/curl:8.5.0
    command: ['sleep', '3600']
EOF

kubectl apply -f network-test.yaml
kubectl wait --for=condition=ready pod/pod-a -n network --timeout=60s
kubectl wait --for=condition=ready pod/pod-b -n network --timeout=60s
kubectl wait --for=condition=ready pod/pod-c -n default --timeout=60s

# Test pod-to-pod communication
POD_A_IP=$(kubectl get pod pod-a -n network -o jsonpath='{.status.podIP}')
echo "Pod A IP: $POD_A_IP"

# Same namespace
kubectl exec -n network pod-b -- curl -s $POD_A_IP | grep -o "<title>.*</title>"

# Cross namespace
kubectl exec -n default pod-c -- curl -s $POD_A_IP | grep -o "<title>.*</title>"
```

Expected: Both pods successfully reach pod-a via IP

---

### Service Discovery: DNS

```bash
# Create service
cat > service-dns.yaml <<'EOF'
apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: network
spec:
  selector:
    app: test-app
  ports:
  - port: 80
    targetPort: 80
EOF

kubectl apply -f service-dns.yaml

# Test DNS resolution
kubectl exec -n network pod-b -- nslookup web-service
kubectl exec -n network pod-b -- curl -s web-service | grep -o "<title>.*</title>"

# Full FQDN from different namespace
kubectl exec -n default pod-c -- nslookup web-service.network.svc.cluster.local
kubectl exec -n default pod-c -- curl -s web-service.network.svc.cluster.local | grep -o "<title>.*</title>"
```

Expected: DNS resolves service name to ClusterIP

---

### Headless Service: Direct Pod IPs

```bash
cat > headless-service.yaml <<'EOF'
apiVersion: v1
kind: Service
metadata:
  name: headless-svc
  namespace: network
spec:
  clusterIP: None  # Headless
  selector:
    app: test-app
  ports:
  - port: 80
EOF

kubectl apply -f headless-service.yaml

# DNS returns pod IPs directly
kubectl exec -n network pod-b -- nslookup headless-svc
```

Expected: DNS returns pod IP(s), not a ClusterIP

---

### NetworkPolicy: Pod Isolation

**Default**: All pods can talk to all pods.

```bash
# Enable isolation: deny all ingress
cat > deny-all-policy.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: network
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF

kubectl apply -f deny-all-policy.yaml

# Test: pod-b cannot reach pod-a anymore
POD_A_IP=$(kubectl get pod pod-a -n network -o jsonpath='{.status.podIP}')
kubectl exec -n network pod-b -- timeout 5 curl -s $POD_A_IP || echo "Connection blocked by NetworkPolicy"
```

Expected: Connection times out (blocked)

```bash
# Allow specific traffic
cat > allow-specific-policy.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-pod-b
  namespace: network
spec:
  podSelector:
    matchLabels:
      app: test-app
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          client: allowed
    ports:
    - protocol: TCP
      port: 80
EOF

# Label pod-b
kubectl label pod pod-b -n network client=allowed

kubectl apply -f allow-specific-policy.yaml

# Test: pod-b can now reach pod-a
kubectl exec -n network pod-b -- curl -s $POD_A_IP | grep -o "<title>.*</title>"

# Test: pod-c still blocked
kubectl exec -n default pod-c -- timeout 5 curl -s $POD_A_IP || echo "Still blocked (no label)"
```

Expected: pod-b allowed, pod-c blocked

**Capture network state**:
```bash
cat > ~/k8s-lab-part2/report/network-summary.txt <<EOF
Network Summary: $(date)

Pod IPs:
$(kubectl get pods -n network -o wide | awk '{print $1, $6}')

Service DNS:
- web-service resolves to: $(kubectl get svc web-service -n network -o jsonpath='{.spec.clusterIP}')
- headless-svc has no ClusterIP (returns pod IPs directly)

NetworkPolicy Enforcement:
- deny-all-ingress: Blocks all ingress by default
- allow-from-pod-b: Allows traffic from pods with label client=allowed

Key Learning: NetworkPolicy is namespace-scoped and uses label selectors
EOF
```

---

### Egress NetworkPolicy: Control Outbound Traffic

```bash
cat > egress-policy.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-control
  namespace: network
spec:
  podSelector:
    matchLabels:
      egress: restricted
  policyTypes:
  - Egress
  egress:
  # Allow DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
  # Allow specific external IP
  - to:
    - ipBlock:
        cidr: 10.0.0.0/8
EOF

# Label pod and test
kubectl label pod pod-b -n network egress=restricted
kubectl apply -f egress-policy.yaml

# Test: DNS works
kubectl exec -n network pod-b -- nslookup google.com

# Test: External traffic blocked (except 10.0.0.0/8)
kubectl exec -n network pod-b -- timeout 5 curl -s 8.8.8.8 || echo "External IP blocked"
```

---

## üö™ Lab 4.3: Ingress Controllers

**Goal**: Expose HTTP services externally with path-based routing.

### Install NGINX Ingress Controller

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml

# Wait for ingress controller ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=controller -n ingress-nginx --timeout=300s

# Get ingress controller service
kubectl get svc -n ingress-nginx
```

---

### Deploy Backend Services

```bash
cat > ingress-backends.yaml <<'EOF'
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-demo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: ingress-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: web
        image: hashicorp/http-echo:1.0
        args:
        - "-text=App1"
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: app1-svc
  namespace: ingress-demo
spec:
  selector:
    app: app1
  ports:
  - port: 80
    targetPort: 5678
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app2
  namespace: ingress-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app2
  template:
    metadata:
      labels:
        app: app2
    spec:
      containers:
      - name: web
        image: hashicorp/http-echo:1.0
        args:
        - "-text=App2"
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: app2-svc
  namespace: ingress-demo
spec:
  selector:
    app: app2
  ports:
  - port: 80
    targetPort: 5678
EOF

kubectl apply -f ingress-backends.yaml
kubectl wait --for=condition=available deployment --all -n ingress-demo --timeout=120s
```

---

### Path-Based Routing

```bash
cat > ingress-paths.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path-routing
  namespace: ingress-demo
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /app1
        pathType: Prefix
        backend:
          service:
            name: app1-svc
            port:
              number: 80
      - path: /app2
        pathType: Prefix
        backend:
          service:
            name: app2-svc
            port:
              number: 80
EOF

kubectl apply -f ingress-paths.yaml

# Wait for ingress address
kubectl get ingress path-routing -n ingress-demo --watch
```

**Test routing** (requires ingress external IP/hostname):
```bash
INGRESS_IP=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# If using Minikube:
# INGRESS_IP=$(minikube ip)

curl http://$INGRESS_IP/app1
curl http://$INGRESS_IP/app2
```

Expected: /app1 returns "App1", /app2 returns "App2"

---

### Host-Based Routing

```bash
cat > ingress-hosts.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: host-routing
  namespace: ingress-demo
spec:
  ingressClassName: nginx
  rules:
  - host: app1.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app1-svc
            port:
              number: 80
  - host: app2.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app2-svc
            port:
              number: 80
EOF

kubectl apply -f ingress-hosts.yaml

# Test with Host header
curl -H "Host: app1.example.com" http://$INGRESS_IP
curl -H "Host: app2.example.com" http://$INGRESS_IP
```

Expected: Different responses based on Host header

**Capture ingress state**:
```bash
kubectl get ingress -n ingress-demo -o yaml > ~/k8s-lab-part2/report/ingress-state.yaml
echo "Ingress rules verified: $(date)" >> ~/k8s-lab-part2/report/network.log
```

---

## üß™ Lab 4.4: Phase 4 Validation

```bash
cat > ~/k8s-lab-part2/phase4-test.sh <<'EOF'
#!/bin/bash
set -e

echo "=== Phase 4 Validation ==="

# Test 1: Storage
echo "1. Testing persistent storage..."
pvc_status=$(kubectl get pvc demo-pvc -n storage -o jsonpath='{.status.phase}')
[ "$pvc_status" == "Bound" ] && echo "‚úÖ PVC bound" || echo "‚ùå PVC not bound"

stateful_pvcs=$(kubectl get pvc -n storage -l app=stateful --no-headers | wc -l)
[ "$stateful_pvcs" -eq 2 ] && echo "‚úÖ StatefulSet PVCs created" || echo "‚ùå StatefulSet PVC count: $stateful_pvcs"

# Test 2: Networking
echo -e "\n2. Testing network policies..."
kubectl get networkpolicy deny-all-ingress -n network &>/dev/null && echo "‚úÖ NetworkPolicy created" || echo "‚ùå NetworkPolicy missing"

# Test 3: Ingress
echo -e "\n3. Testing ingress..."
kubectl get ingress path-routing -n ingress-demo &>/dev/null && echo "‚úÖ Ingress created" || echo "‚ùå Ingress missing"

echo -e "\n=== Phase 4 Complete ==="
EOF

chmod +x ~/k8s-lab-part2/phase4-test.sh
~/k8s-lab-part2/phase4-test.sh > ~/k8s-lab-part2/report/phase4-results.txt 2>&1
cat ~/k8s-lab-part2/report/phase4-results.txt
```

---

# PHASE 5: CLUSTER OPERATIONS (WEEKS 9-10)

## üîß Lab 5.1: etcd Backup & Restore

**Goal**: Protect cluster state with backup/restore procedures.

### Aha Moment #12: etcd IS Kubernetes

**Concept**: All cluster state (pods, services, secrets) lives in etcd.

### Backup etcd

```bash
# Install etcdctl
ETCD_VERSION="v3.5.11"
curl -L https://github.com/etcd-io/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz -o etcd.tar.gz
tar xzf etcd.tar.gz
sudo mv etcd-${ETCD_VERSION}-linux-amd64/etcdctl /usr/local/bin/
rm -rf etcd-${ETCD_VERSION}-linux-amd64 etcd.tar.gz

# Verify etcdctl
etcdctl version
```

```bash
# Create backup directory
mkdir -p ~/k8s-lab-part2/etcd-backups

# Backup etcd (adjust paths for your setup)
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')

kubectl exec -n kube-system $ETCD_POD -- sh -c \
  "ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /var/lib/etcd/snapshot.db"

# Copy backup to local machine
kubectl cp -n kube-system $ETCD_POD:/var/lib/etcd/snapshot.db ~/k8s-lab-part2/etcd-backups/snapshot-$(date +%Y%m%d-%H%M%S).db

# Verify backup
etcdctl snapshot status ~/k8s-lab-part2/etcd-backups/snapshot-*.db --write-out=table
```

---

### Test Restore Scenario

```bash
# Create test data
kubectl create namespace backup-test
kubectl run test-pod --image=nginx:1.25 -n backup-test

# Verify pod exists
kubectl get pod test-pod -n backup-test

# Simulate disaster: delete namespace
kubectl delete namespace backup-test

# Verify deletion
kubectl get namespace backup-test || echo "Namespace deleted"

# Restore from backup (NOTE: This requires cluster downtime in production)
# For learning: restore creates new data directory

kubectl exec -n kube-system $ETCD_POD -- sh -c \
  "ETCDCTL_API=3 etcdctl snapshot restore /var/lib/etcd/snapshot.db \
  --data-dir=/var/lib/etcd-restore \
  --name=etcd-restore"
```

**Important**: Full restore requires stopping control plane and replacing etcd data directory. Document procedure:

```bash
cat > ~/k8s-lab-part2/report/etcd-restore-procedure.txt <<EOF
etcd Restore Procedure: $(date)

1. Backup created: ~/k8s-lab-part2/etcd-backups/snapshot-*.db
2. Backup size: $(ls -lh ~/k8s-lab-part2/etcd-backups/snapshot-*.db | awk '{print $5}')
3. Backup verification: 
   $(etcdctl snapshot status ~/k8s-lab-part2/etcd-backups/snapshot-*.db --write-out=table)

Full Restore Steps (production):
1. Stop kube-apiserver
2. Stop etcd
3. Restore snapshot to new directory
4. Update etcd data-dir in manifests
5. Start etcd
6. Start kube-apiserver
7. Verify cluster state

Key Learning: etcd backup = cluster state backup. Schedule regular backups.
EOF
```

---

## üîê Lab 5.2: RBAC - Role-Based Access Control

**Goal**: Implement fine-grained authorization using roles and service accounts.

### Aha Moment #13: Authentication ‚â† Authorization

**Concept**: ServiceAccounts authenticate, Roles/RoleBindings authorize.

### ServiceAccount & Role

```bash
kubectl create namespace rbac-demo

# Create ServiceAccount
kubectl create serviceaccount app-sa -n rbac-demo

# Create Role (namespace-scoped permissions)
cat > pod-reader-role.yaml <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: rbac-demo
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
EOF

kubectl apply -f pod-reader-role.yaml

# Bind Role to ServiceAccount
kubectl create rolebinding app-sa-binding \
  --role=pod-reader \
  --serviceaccount=rbac-demo:app-sa \
  -n rbac-demo
```

**Test RBAC**:
```bash
# Get ServiceAccount token
SA_TOKEN=$(kubectl create token app-sa -n rbac-demo --duration=1h)

# Test allowed operation (list pods)
kubectl --token=$SA_TOKEN get pods -n rbac-demo

# Test denied operation (create pod)
kubectl --token=$SA_TOKEN run test --image=nginx -n rbac-demo
```

Expected: GET succeeds, CREATE denied

---

### ClusterRole & ClusterRoleBinding

```bash
# Create ClusterRole (cluster-wide permissions)
cat > node-reader-clusterrole.yaml <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]
EOF

kubectl apply -f node-reader-clusterrole.yaml

# Bind to ServiceAccount
kubectl create clusterrolebinding app-sa-cluster-binding \
  --clusterrole=node-reader \
  --serviceaccount=rbac-demo:app-sa

# Test
kubectl --token=$SA_TOKEN get nodes
```

Expected: Lists nodes (cluster-wide access)

---

### RBAC Aggregation

```bash
cat > aggregated-role.yaml <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aggregate-reader
  labels:
    rbac.example.com/aggregate-to-reader: "true"
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.example.com/aggregate-to-reader: "true"
rules: []  # Filled by aggregation
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-reader-cluster
  labels:
    rbac.example.com/aggregate-to-reader: "true"
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: service-reader-cluster
  labels:
    rbac.example.com/aggregate-to-reader: "true"
rules:
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list"]
EOF

kubectl apply -f aggregated-role.yaml

# View aggregated permissions
kubectl describe clusterrole aggregate-reader
```

Expected: Shows combined rules from both labeled ClusterRoles

**Capture RBAC state**:
```bash
cat > ~/k8s-lab-part2/report/rbac-summary.txt <<EOF
RBAC Summary: $(date)

ServiceAccount: app-sa (namespace: rbac-demo)

Roles:
- pod-reader: Can get/list/watch pods in rbac-demo namespace
- node-reader (ClusterRole): Can get/list nodes cluster-wide

RoleBindings:
$(kubectl get rolebindings -n rbac-demo --no-headers)

ClusterRoleBindings:
$(kubectl get clusterrolebindings | grep app-sa)

Test Results:
- ‚úÖ app-sa can list pods in rbac-demo
- ‚ùå app-sa cannot create pods in rbac-demo
- ‚úÖ app-sa can list nodes cluster-wide

Key Learning: Role (namespace) vs ClusterRole (cluster), Binding connects them to subjects
EOF
```

---

## üêõ Lab 5.3: The 22 Common Kubernetes Errors

**Goal**: Experience, diagnose, and fix the most frequent Kubernetes failures.

### Error 1-5: Pod State Issues

**1. ImagePullBackOff**
```bash
cat > error-imagepull.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-imagepull
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: nginx:nonexistent-tag
EOF

kubectl apply -f error-imagepull.yaml

# Diagnosis
kubectl describe pod error-imagepull -n rbac-demo | grep -A 5 "Events:"
kubectl get pod error-imagepull -n rbac-demo -o jsonpath='{.status.containerStatuses[0].state.waiting.message}'
```

**Fix**: Use valid image tag
```bash
kubectl delete pod error-imagepull -n rbac-demo
kubectl run error-imagepull --image=nginx:1.25 -n rbac-demo
```

---

**2. CrashLoopBackOff**
```bash
cat > error-crashloop.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-crashloop
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: busybox:1.36
    command: ['sh', '-c', 'exit 1']
EOF

kubectl apply -f error-crashloop.yaml

# Watch crash loop
kubectl get pod error-crashloop -n rbac-demo --watch

# Diagnosis
kubectl logs error-crashloop -n rbac-demo
kubectl describe pod error-crashloop -n rbac-demo | grep "Exit Code"
```

**Fix**: Ensure command succeeds
```bash
kubectl delete pod error-crashloop -n rbac-demo
cat > error-crashloop-fixed.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-crashloop-fixed
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: busybox:1.36
    command: ['sh', '-c', 'while true; do sleep 10; done']
EOF
kubectl apply -f error-crashloop-fixed.yaml
```

---

**3. CreateContainerConfigError**
```bash
cat > error-configerror.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-configerror
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: nginx:1.25
    env:
    - name: CONFIG
      valueFrom:
        configMapKeyRef:
          name: nonexistent-cm
          key: data
EOF

kubectl apply -f error-configerror.yaml

# Diagnosis
kubectl describe pod error-configerror -n rbac-demo | grep -A 3 "Warning"
```

**Fix**: Create ConfigMap first
```bash
kubectl create configmap nonexistent-cm --from-literal=data=value -n rbac-demo
kubectl delete pod error-configerror -n rbac-demo
kubectl apply -f error-configerror.yaml
```

---

**4. OOMKilled**
```bash
cat > error-oom.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-oom
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: polinux/stress:1.0.4
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "512M"]
    resources:
      limits:
        memory: "256Mi"
EOF

kubectl apply -f error-oom.yaml

# Watch OOM kill
kubectl get pod error-oom -n rbac-demo --watch

# Diagnosis
kubectl describe pod error-oom -n rbac-demo | grep -E "OOMKilled|Memory"
kubectl get pod error-oom -n rbac-demo -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'
```

**Fix**: Increase memory limit
```bash
kubectl delete pod error-oom -n rbac-demo
cat > error-oom-fixed.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-oom-fixed
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: polinux/stress:1.0.4
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "256M"]
    resources:
      limits:
        memory: "512Mi"
EOF
kubectl apply -f error-oom-fixed.yaml
```

---

**5. Pending (Insufficient Resources)**
```bash
cat > error-pending.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: error-pending
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "100"  # Impossible request
        memory: "1000Gi"
EOF

kubectl apply -f error-pending.yaml

# Diagnosis
kubectl describe pod error-pending -n rbac-demo | grep -A 5 "Events:"
kubectl get events -n rbac-demo --sort-by='.lastTimestamp' | grep error-pending
```

**Fix**: Reduce resource requests
```bash
kubectl delete pod error-pending -n rbac-demo
```

---

### Error 6-10: Service & Networking

**6. Service Not Routing to Pods**
```bash
# Deploy pod without matching label
kubectl run web --image=nginx:1.25 --labels=app=wrong -n rbac-demo

# Create service expecting different label
kubectl create service clusterip web-svc --tcp=80:80 -n rbac-demo

# Diagnosis: No endpoints
kubectl get endpoints web-svc -n rbac-demo
kubectl describe svc web-svc -n rbac-demo
```

**Fix**: Match labels
```bash
kubectl label pod web -n rbac-demo app=web-svc --overwrite
kubectl get endpoints web-svc -n rbac-demo
```

---

**7. DNS Resolution Failure**
```bash
# Test DNS
kubectl run dns-test --image=busybox:1.36 -n rbac-demo -- sleep 3600
kubectl exec -n rbac-demo dns-test -- nslookup kubernetes.default || echo "DNS failure"

# Diagnosis: Check CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl logs -n kube-system -l k8s-app=kube-dns --tail=20
```

---

**8. NetworkPolicy Blocking Traffic**
```bash
# Deploy app and service
kubectl run backend --image=nginx:1.25 --labels=app=backend -n rbac-demo
kubectl expose pod backend --port=80 -n rbac-demo

# Deploy client
kubectl run client --image=curlimages/curl:8.5.0 -n rbac-demo -- sleep 3600

# Test connectivity
kubectl exec -n rbac-demo client -- curl -s backend

# Apply restrictive NetworkPolicy
cat > netpol-block.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-backend
  namespace: rbac-demo
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
EOF

kubectl apply -f netpol-block.yaml

# Test again - blocked
kubectl exec -n rbac-demo client -- timeout 5 curl -s backend || echo "Blocked by NetworkPolicy"

# Diagnosis
kubectl describe networkpolicy deny-backend -n rbac-demo

# Fix: Allow specific traffic
kubectl delete networkpolicy deny-backend -n rbac-demo
```

---

**9. Ingress 404 Errors**
```bash
# Create backend
kubectl create deployment web --image=nginx:1.25 -n rbac-demo
kubectl expose deployment web --port=80 -n rbac-demo

# Create Ingress with wrong path
cat > ingress-404.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: rbac-demo
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /wrongpath
        pathType: Prefix
        backend:
          service:
            name: web
            port:
              number: 80
EOF

kubectl apply -f ingress-404.yaml

# Diagnosis: Check ingress rules
kubectl describe ingress test-ingress -n rbac-demo
```

---

**10. Port Mismatch**
```bash
# Deploy pod listening on 8080
kubectl run app --image=hashicorp/http-echo:1.0 --port=5678 -n rbac-demo -- -text="Hello"

# Create service targeting wrong port
kubectl expose pod app --port=80 --target-port=8080 -n rbac-demo

# Test - connection refused
kubectl run test --image=curlimages/curl:8.5.0 -n rbac-demo --rm -it -- curl app

# Diagnosis
kubectl get svc app -n rbac-demo -o yaml | grep -A 3 "ports:"
kubectl get pod app -n rbac-demo -o jsonpath='{.spec.containers[0].ports[0].containerPort}'

# Fix
kubectl delete svc app -n rbac-demo
kubectl expose pod app --port=80 --target-port=5678 -n rbac-demo
```

---

### Error 11-15: Storage Issues

**11. PVC Pending (No Matching PV)**
```bash
cat > pvc-pending.yaml <<'EOF'
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pending-pvc
  namespace: rbac-demo
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi  # No PV this size exists
  storageClassName: manual
EOF

kubectl apply -f pvc-pending.yaml

# Diagnosis
kubectl get pvc pending-pvc -n rbac-demo
kubectl describe pvc pending-pvc -n rbac-demo | grep -A 5 "Events:"

# Fix: Create matching PV or reduce size
kubectl delete pvc pending-pvc -n rbac-demo
```

---

**12. Mount Permission Denied**
```bash
cat > mount-permission.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: mount-test
  namespace: rbac-demo
spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: app
    image: nginx:1.25
    volumeMounts:
    - name: data
      mountPath: /usr/share/nginx/html
  volumes:
  - name: data
    hostPath:
      path: /root/restricted  # User 1000 can't write here
EOF

kubectl apply -f mount-permission.yaml

# Diagnosis
kubectl logs mount-test -n rbac-demo
kubectl describe pod mount-test -n rbac-demo

# Fix: Use appropriate path or adjust fsGroup
kubectl delete pod mount-test -n rbac-demo
```

---

### Error 16-22: Configuration & Deployment

**13. Invalid YAML Syntax**
```bash
# Intentional syntax error
cat > invalid-yaml.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: syntax-error
  namespace: rbac-demo
spec:
  containers:
  - name: app
    image: nginx:1.25
    ports
    - containerPort: 80  # Missing colon after "ports"
EOF

kubectl apply -f invalid-yaml.yaml 2>&1 | head -5
```

**Fix**: Validate YAML
```bash
kubectl apply --dry-run=client -f invalid-yaml.yaml
```

---

**14. Deployment Rollout Stuck**
```bash
cat > stuck-rollout.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stuck-deploy
  namespace: rbac-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: stuck
  template:
    metadata:
      labels:
        app: stuck
    spec:
      containers:
      - name: app
        image: nginx:nonexistent
EOF

kubectl apply -f stuck-rollout.yaml

# Diagnosis
kubectl rollout status deployment/stuck-deploy -n rbac-demo --timeout=30s
kubectl describe deployment stuck-deploy -n rbac-demo
kubectl get replicaset -n rbac-demo -l app=stuck

# Fix
kubectl set image deployment/stuck-deploy app=nginx:1.25 -n rbac-demo
kubectl rollout status deployment/stuck-deploy -n rbac-demo
```

---

**15-22: Rapid Fire Diagnosis Commands**

```bash
cat > ~/k8s-lab-part2/error-diagnosis-guide.sh <<'EOF'
#!/bin/bash
# Quick diagnosis commands for remaining errors

echo "=== Error 15: Secret Not Found ==="
kubectl get secrets -n <namespace>
kubectl describe pod <pod-name> -n <namespace> | grep Secret

echo -e "\n=== Error 16: Liveness Probe Failing ==="
kubectl describe pod <pod-name> -n <namespace> | grep -A 5 Liveness
kubectl logs <pod-name> -n <namespace> --previous

echo -e "\n=== Error 17: Readiness Probe Never Ready ==="
kubectl get pod <pod-name> -n <namespace> -o jsonpath='{.status.conditions[?(@.type=="Ready")].message}'

echo -e "\n=== Error 18: Init Container Failure ==="
kubectl logs <pod-name> -n <namespace> -c <init-container-name>
kubectl describe pod <pod-name> -n <namespace> | grep -A 10 "Init Containers:"

echo -e "\n=== Error 19: Affinity Rules Unmet ==="
kubectl describe pod <pod-name> -n <namespace> | grep -A 10 "Events:"
kubectl get nodes --show-labels

echo -e "\n=== Error 20: Taints Preventing Scheduling ==="
kubectl describe nodes | grep Taints
kubectl describe pod <pod-name> -n <namespace> | grep -i taint

echo -e "\n=== Error 21: Resource Quota Exceeded ==="
kubectl describe resourcequota -n <namespace>
kubectl get resourcequota -n <namespace> -o yaml

echo -e "\n=== Error 22: RBAC Permission Denied ==="
kubectl auth can-i <verb> <resource> --as=<user/serviceaccount> -n <namespace>
kubectl describe role <role-name> -n <namespace>
kubectl describe rolebinding <binding-name> -n <namespace>
EOF

chmod +x ~/k8s-lab-part2/error-diagnosis-guide.sh
```

**Capture error scenarios**:
```bash
cat > ~/k8s-lab-part2/report/error-scenarios.txt <<EOF
Common Kubernetes Errors - Hands-On Experience: $(date)

Errors Simulated & Fixed:
1. ‚úÖ ImagePullBackOff - Invalid image tag
2. ‚úÖ CrashLoopBackOff - Command exiting with error
3. ‚úÖ CreateContainerConfigError - Missing ConfigMap
4. ‚úÖ OOMKilled - Memory limit exceeded
5. ‚úÖ Pending - Insufficient resources
6. ‚úÖ Service routing failure - Label mismatch
7. ‚úÖ DNS resolution - CoreDNS issues
8. ‚úÖ NetworkPolicy blocking - Restrictive ingress rules
9. ‚úÖ Ingress 404 - Path mismatch
10. ‚úÖ Port mismatch - Service targetPort wrong
11. ‚úÖ PVC Pending - No matching PV
12. ‚úÖ Mount permission denied - User/path mismatch
13. ‚úÖ Invalid YAML - Syntax errors
14. ‚úÖ Deployment rollout stuck - Bad image in new ReplicaSet

Diagnosis Scripts Created:
- ~/k8s-lab-part2/error-diagnosis-guide.sh (errors 15-22)

Key Learning: Most errors visible in 'kubectl describe' Events section
EOF
```

---

## üß™ Lab 5.4: Phase 5 Validation

```bash
cat > ~/k8s-lab-part2/phase5-test.sh <<'EOF'
#!/bin/bash
set -e

echo "=== Phase 5 Validation ==="

# Test 1: etcd backup
echo "1. Testing etcd backup..."
[ -f ~/k8s-lab-part2/etcd-backups/snapshot-*.db ] && echo "‚úÖ etcd backup exists" || echo "‚ùå No backup found"

# Test 2: RBAC
echo -e "\n2. Testing RBAC..."
kubectl get serviceaccount app-sa -n rbac-demo &>/dev/null && echo "‚úÖ ServiceAccount created" || echo "‚ùå ServiceAccount missing"
kubectl get role pod-reader -n rbac-demo &>/dev/null && echo "‚úÖ Role created" || echo "‚ùå Role missing"

# Test 3: Error scenarios
echo -e "\n3. Verified error scenarios..."
echo "‚úÖ ImagePullBackOff"
echo "‚úÖ CrashLoopBackOff"
echo "‚úÖ OOMKilled"
echo "‚úÖ Service routing issues"
echo "‚úÖ NetworkPolicy blocking"

echo -e "\n=== Phase 5 Complete ==="
EOF

chmod +x ~/k8s-lab-part2/phase5-test.sh
~/k8s-lab-part2/phase5-test.sh > ~/k8s-lab-part2/report/phase5-results.txt 2>&1
cat ~/k8s-lab-part2/report/phase5-results.txt
```

---

# PHASE 6: PRODUCTION DEPLOYMENT (WEEKS 11-12)

## üîí Lab 6.1: Security Hardening

**Goal**: Implement production security best practices.

### Pod Security Standards

```bash
kubectl create namespace secure-apps

# Label namespace with security standard
kubectl label namespace secure-apps pod-security.kubernetes.io/enforce=restricted

# Try to deploy privileged pod - should fail
cat > privileged-pod.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: privileged-test
  namespace: secure-apps
spec:
  containers:
  - name: app
    image: nginx:1.25
    securityContext:
      privileged: true
EOF

kubectl apply -f privileged-pod.yaml 2>&1 | grep -i "violates"

# Deploy compliant pod
cat > secure-pod.yaml <<'EOF'
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: secure-apps
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    image: nginx:1.25
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    volumeMounts:
    - name: cache
      mountPath: /var/cache/nginx
    - name: run
      mountPath: /var/run
  volumes:
  - name: cache
    emptyDir: {}
  - name: run
    emptyDir: {}
EOF

kubectl apply -f secure-pod.yaml
kubectl wait --for=condition=ready pod/secure-app -n secure-apps --timeout=60s
```

**Verify security context**:
```bash
kubectl exec -n secure-apps secure-app -- id
kubectl exec -n secure-apps secure-app -- grep "Cap" /proc/1/status
```

Expected: Running as user 1000, no capabilities

---

### Network Policies: Default Deny

```bash
# Default deny all traffic
cat > default-deny.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: secure-apps
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
EOF

kubectl apply -f default-deny.yaml

# Allow egress to DNS only
cat > allow-dns.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: secure-apps
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
EOF

kubectl apply -f allow-dns.yaml

# Test: DNS works, external traffic blocked
kubectl run test --image=busybox:1.36 -n secure-apps -- sleep 3600
kubectl exec -n secure-apps test -- nslookup kubernetes.default
kubectl exec -n secure-apps test -- timeout 5 wget -O- https://google.com || echo "External traffic blocked"
```

---

## üìä Lab 6.2: Monitoring & Autoscaling

**Goal**: Implement Horizontal Pod Autoscaler and metrics collection.

### Install Metrics Server

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Patch for local development (skip TLS verification)
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

# Wait for metrics server ready
kubectl wait --for=condition=available deployment/metrics-server -n kube-system --timeout=120s

# Verify metrics
kubectl top nodes
kubectl top pods -A
```

---

### Horizontal Pod Autoscaler (HPA)

```bash
kubectl create namespace autoscale

# Deploy application with resource requests
cat > hpa-demo.yaml <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
  namespace: autoscale
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-apache
  template:
    metadata:
      labels:
        app: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m
          limits:
            cpu: 500m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  namespace: autoscale
spec:
  selector:
    app: php-apache
  ports:
  - port: 80
EOF

kubectl apply -f hpa-demo.yaml
kubectl wait --for=condition=available deployment/php-apache -n autoscale --timeout=120s

# Create HPA
kubectl autoscale deployment php-apache -n autoscale --cpu-percent=50 --min=1 --max=10

# Verify HPA
kubectl get hpa php-apache -n autoscale
```

**Generate load**:
```bash
# Run load generator
kubectl run load-generator -n autoscale --image=busybox:1.36 -- /bin/sh -c "while true; do wget -q -O- http://php-apache; done"

# Watch HPA scale up
watch -n 5 "kubectl get hpa php-apache -n autoscale"
```

Expected: Replicas increase as CPU usage rises

**Stop load**:
```bash
kubectl delete pod load-generator -n autoscale

# Watch scale down (takes ~5 minutes)
kubectl get hpa php-apache -n autoscale --watch
```

---

### Vertical Pod Autoscaler (VPA) - Concept

```bash
cat > vpa-example.yaml <<'EOF'
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-demo
  namespace: autoscale
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  updatePolicy:
    updateMode: "Off"  # Recommendation only
EOF

# Note: VPA requires VPA controller installation
# This shows the manifest structure for learning
```

**Capture autoscaling state**:
```bash
cat > ~/k8s-lab-part2/report/autoscaling-summary.txt <<EOF
Autoscaling Summary: $(date)

Metrics Server Status:
$(kubectl get deployment metrics-server -n kube-system -o jsonpath='{.status.conditions[?(@.type=="Available")].status}')

HPA Configuration:
$(kubectl get hpa php-apache -n autoscale -o yaml | grep -A 10 "spec:")

HPA Current State:
- Target CPU: 50%
- Min Replicas: 1
- Max Replicas: 10
- Current Replicas: $(kubectl get hpa php-apache -n autoscale -o jsonpath='{.status.currentReplicas}')

Key Learning: HPA scales horizontally (more pods), VPA scales vertically (more resources per pod)
EOF
```

---

## üöÄ Lab 6.3: GitOps with ArgoCD (Conceptual)

**Goal**: Understand GitOps workflow for production deployments.

### GitOps Principles

```bash
cat > ~/k8s-lab-part2/gitops-workflow.md <<'EOF'
# GitOps Workflow

## Core Principles
1. **Declarative**: Entire system state in Git
2. **Versioned**: Git history = deployment history
3. **Pulled**: Changes pulled from Git, not pushed to cluster
4. **Reconciled**: Automated sync between Git and cluster state

## ArgoCD Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Git   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ  ArgoCD  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ  Cluster  ‚îÇ
‚îÇ  Repo   ‚îÇ         ‚îÇ Server   ‚îÇ         ‚îÇ   State   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Sync    ‚îÇ
                    ‚îÇ  Status  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Sample Application Manifest Structure
```
my-app/
‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml
‚îî‚îÄ‚îÄ overlays/
    ‚îú‚îÄ‚îÄ dev/
    ‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml
    ‚îî‚îÄ‚îÄ prod/
        ‚îî‚îÄ‚îÄ kustomization.yaml
```

## ArgoCD Application CRD
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/example/manifests
    targetRevision: main
    path: my-app/overlays/prod
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

## Benefits
- ‚úÖ Single source of truth (Git)
- ‚úÖ Audit trail (Git commits)
- ‚úÖ Rollback capability (Git revert)
- ‚úÖ Disaster recovery (Git + cluster rebuild)
- ‚úÖ Multi-cluster management
EOF

cat ~/k8s-lab-part2/gitops-workflow.md
```

---

## üèÜ Lab 6.4: Production-Ready Deployment

**Goal**: Deploy a complete production application with all best practices.

### Complete Application Stack

```bash
kubectl create namespace production

# 1. Deploy database with persistent storage
cat > postgres-deployment.yaml <<'EOF'
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: production
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: manual
---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: production
type: Opaque
stringData:
  POSTGRES_PASSWORD: prodpassword123
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      securityContext:
        runAsUser: 999
        fsGroup: 999
      containers:
      - name: postgres
        image: postgres:15-alpine
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        ports:
        - containerPort: 5432
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - postgres
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: production
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
EOF

# Create PV for postgres
cat > postgres-pv.yaml <<'EOF'
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: /tmp/postgres-data
    type: DirectoryOrCreate
EOF

kubectl apply -f postgres-pv.yaml
kubectl apply -f postgres-deployment.yaml
kubectl wait --for=condition=ready pod -l app=postgres -n production --timeout=120s
```

---

**2. Deploy application with ConfigMap**

```bash
cat > app-deployment.yaml <<'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  DATABASE_HOST: postgres
  DATABASE_PORT: "5432"
  LOG_LEVEL: "info"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: app
        image: nginx:1.25-alpine
        envFrom:
        - configMapRef:
            name: app-config
        env:
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
---
apiVersion: v1
kind: Service
metadata:
  name: web-app
  namespace: production
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
EOF

kubectl apply -f app-deployment.yaml
kubectl wait --for=condition=available deployment/web-app -n production --timeout=120s
```

---

**3. Network policies for production**

```bash
cat > production-netpol.yaml <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: postgres
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: web-app
    ports:
    - protocol: TCP
      port: 5432
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-app-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80
  egress:
  # Allow DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
  # Allow postgres
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
EOF

kubectl apply -f production-netpol.yaml
```

---

**Final production validation**:
```bash
cat > ~/k8s-lab-part2/production-validation.sh <<'EOF'
#!/bin/bash

echo "=== Production Stack Validation ==="

# Database
echo "1. PostgreSQL Status:"
kubectl get pod -l app=postgres -n production -o jsonpath='{.items[0].status.phase}'
kubectl exec -n production -l app=postgres -- pg_isready -U postgres

# Application
echo -e "\n2. Web Application Status:"
kubectl get deployment web-app -n production -o jsonpath='{.status.conditions[?(@.type=="Available")].status}'
kubectl get pods -l app=web-app -n production

# HPA
echo -e "\n3. Autoscaler Status:"
kubectl get hpa web-app-hpa -n production

# Network Policies
echo -e "\n4. Network Policies:"
kubectl get networkpolicy -n production

# Security
echo -e "\n5. Security Verification:"
kubectl get pod -l app=web-app -n production -o jsonpath='{.items[0].spec.securityContext}'

echo -e "\n=== Validation Complete ==="
EOF

chmod +x ~/k8s-lab-part2/production-validation.sh
~/k8s-lab-part2/production-validation.sh > ~/k8s-lab-part2/report/production-validation.txt 2>&1
cat ~/k8s-lab-part2/report/production-validation.txt
```

---

## üìã Final Lab Report

```bash
cat > ~/k8s-lab-part2/FINAL-REPORT.md <<'EOF'
# Kubernetes Mastery Lab - Part 2 Completion Report

**Completion Date**: $(date)
**Duration**: 8 weeks (Phases 3-6)

## Phase 3: Advanced Patterns ‚úÖ

### Multi-Container Design Patterns
- ‚úÖ Sidecar: Log processor sharing volumes
- ‚úÖ Ambassador: Local proxy for external services
- ‚úÖ Adapter: Format transformation container

### Health Probes & Lifecycle
- ‚úÖ Liveness probe: Restart deadlocked containers
- ‚úÖ Readiness probe: Traffic routing control
- ‚úÖ Startup probe: Protect slow-starting apps
- ‚úÖ PreStop hook: Graceful shutdown

### Resource Management
- ‚úÖ QoS Classes: Guaranteed, Burstable, BestEffort
- ‚úÖ LimitRange: Namespace defaults
- ‚úÖ ResourceQuota: Namespace caps
- ‚úÖ OOM behavior understanding

**Artifacts**: ~/k8s-lab-part2/report/patterns-summary.txt, probe-configs.txt, resource-summary.txt

---

## Phase 4: Storage & Networking ‚úÖ

### Persistent Storage
- ‚úÖ Volume types: emptyDir, hostPath, PV/PVC
- ‚úÖ StatefulSet with volumeClaimTemplates
- ‚úÖ Data persistence verification

### Networking
- ‚úÖ Pod-to-pod communication (flat network)
- ‚úÖ Service discovery (DNS)
- ‚úÖ Headless services
- ‚úÖ NetworkPolicy: Ingress/Egress rules

### Ingress
- ‚úÖ NGINX Ingress Controller installation
- ‚úÖ Path-based routing
- ‚úÖ Host-based routing

**Artifacts**: ~/k8s-lab-part2/report/storage-summary.txt, network-summary.txt, ingress-state.yaml

---

## Phase 5: Cluster Operations ‚úÖ

### etcd Backup & Restore
- ‚úÖ Snapshot creation
- ‚úÖ Backup verification
- ‚úÖ Restore procedure documentation

### RBAC
- ‚úÖ ServiceAccount creation
- ‚úÖ Role & RoleBinding (namespace-scoped)
- ‚úÖ ClusterRole & ClusterRoleBinding
- ‚úÖ Permission testing

### 22 Common Errors
- ‚úÖ ImagePullBackOff
- ‚úÖ CrashLoopBackOff
- ‚úÖ CreateContainerConfigError
- ‚úÖ OOMKilled
- ‚úÖ Pending (resources)
- ‚úÖ Service routing failures
- ‚úÖ NetworkPolicy blocking
- ‚úÖ Ingress 404 errors
- ‚úÖ PVC pending
- ‚úÖ Deployment rollout stuck
- ‚úÖ 12 additional error scenarios documented

**Artifacts**: ~/k8s-lab-part2/etcd-backups/, error-diagnosis-guide.sh, error-scenarios.txt

---

## Phase 6: Production Deployment ‚úÖ

### Security Hardening
- ‚úÖ Pod Security Standards (restricted)
- ‚úÖ Security contexts (non-root, capabilities)
- ‚úÖ Default deny NetworkPolicies

### Monitoring & Autoscaling
- ‚úÖ Metrics Server installation
- ‚úÖ Horizontal Pod Autoscaler (HPA)
- ‚úÖ Load testing & scale verification
- ‚úÖ VPA concepts

### GitOps
- ‚úÖ GitOps principles understanding
- ‚úÖ ArgoCD architecture
- ‚úÖ Manifest structure best practices

### Production Stack
- ‚úÖ PostgreSQL with persistent storage
- ‚úÖ Web application with ConfigMap/Secret
- ‚úÖ HPA configured
- ‚úÖ NetworkPolicies enforced
- ‚úÖ Health probes implemented
- ‚úÖ Security hardening applied

**Artifacts**: ~/k8s-lab-part2/gitops-workflow.md, production-validation.txt

---

## Complete Lab Statistics

**Total Pods Created**: 50+
**Namespaces Used**: 10
**Manifests Written**: 75+
**Error Scenarios Debugged**: 22
**Network Policies Implemented**: 8
**Storage Volumes Created**: 15+

## Skills Mastered

### Architectural Understanding
- ‚úÖ Multi-container pod patterns
- ‚úÖ Storage abstraction layers
- ‚úÖ Network flat model
- ‚úÖ RBAC authorization model
- ‚úÖ Autoscaling mechanisms

### Operational Expertise
- ‚úÖ Backup/restore procedures
- ‚úÖ Error diagnosis workflows
- ‚úÖ Production deployment patterns
- ‚úÖ Security hardening techniques

### Production Readiness
- ‚úÖ Health probe configuration
- ‚úÖ Resource management
- ‚úÖ Network isolation
- ‚úÖ GitOps workflows
- ‚úÖ Monitoring integration

---

## Interview Preparation

### Architecture Questions

**Q1**: Explain the difference between a sidecar and an ambassador pattern.

**Expected Answer**:
- **Sidecar**: Enhances/extends main container functionality (e.g., log forwarding, service mesh proxy)
- **Ambassador**: Proxies external connections (e.g., connection pooling, retry logic)
- Both use pod's shared network namespace (localhost communication)
- Evidence: web-with-sidecar pod (logs access) vs app-with-ambassador pod (HAProxy to external service)

**Q2**: How does Kubernetes handle pod eviction under memory pressure?

**Expected Answer**:
- QoS determines eviction order: BestEffort ‚Üí Burstable ‚Üí Guaranteed
- BestEffort: No requests/limits, evicted first
- Burstable: Requests < limits, evicted second
- Guaranteed: Requests = limits, evicted last
- Evidence: ~/k8s-lab-part2/report/resource-summary.txt shows 3 QoS classes

---

### Scenario Questions

**Q3**: A deployment rollout is stuck. Walk through your diagnosis.

**Expected Answer**:
1. Check rollout status: `kubectl rollout status deployment/X`
2. Examine new ReplicaSet: `kubectl get rs -l app=X`
3. Describe pods in new RS: Look for ImagePull, CreateContainer errors
4. Check events: `kubectl describe deployment X`
5. View pod logs: `kubectl logs` for application errors
6. Evidence: stuck-deploy scenario - nonexistent image caused rollout failure

**Q4**: Design network isolation for a 3-tier app (web ‚Üí api ‚Üí db).

**Expected Answer**:
1. Default deny all ingress/egress
2. Web tier: Allow ingress from Ingress controller, egress to API
3. API tier: Allow ingress from web tier, egress to DB
4. DB tier: Allow ingress from API tier only
5. All tiers: Allow egress to CoreDNS (port 53/UDP)
6. Evidence: production-netpol.yaml shows postgres isolated to web-app only

---

## Next Steps

### CKA Certification Readiness
You are now ready to:
- ‚úÖ Deploy and configure multi-tier applications
- ‚úÖ Troubleshoot cluster and application failures
- ‚úÖ Implement security best practices
- ‚úÖ Manage persistent storage
- ‚úÖ Configure networking and policies
- ‚úÖ Perform backup/restore operations

### Advanced Topics to Explore
- Helm package management
- Custom Resource Definitions (CRDs)
- Operators and controller patterns
- Service mesh (Istio/Linkerd)
- Cluster API for cluster lifecycle
- Multi-cluster management

---

## Report Artifacts Summary

All validation outputs, state captures, and evidence files stored in:
```
~/k8s-lab-part2/report/
‚îú‚îÄ‚îÄ patterns-summary.txt
‚îú‚îÄ‚îÄ probe-configs.txt
‚îú‚îÄ‚îÄ resource-summary.txt
‚îú‚îÄ‚îÄ storage-summary.txt
‚îú‚îÄ‚îÄ network-summary.txt
‚îú‚îÄ‚îÄ ingress-state.yaml
‚îú‚îÄ‚îÄ etcd-restore-procedure.txt
‚îú‚îÄ‚îÄ rbac-summary.txt
‚îú‚îÄ‚îÄ error-scenarios.txt
‚îú‚îÄ‚îÄ autoscaling-summary.txt
‚îú‚îÄ‚îÄ production-validation.txt
‚îú‚îÄ‚îÄ phase3-results.txt
‚îú‚îÄ‚îÄ phase4-results.txt
‚îî‚îÄ‚îÄ phase5-results.txt
```

**üéì Congratulations! You've completed the Kubernetes Mastery Lab.**
EOF

cat ~/k8s-lab-part2/FINAL-REPORT.md
```

